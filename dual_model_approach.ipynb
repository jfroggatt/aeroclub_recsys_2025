{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ],
   "id": "1c7b783dc30d7afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Customer attributes for clustering analysis\n",
    "# CUSTOMER_ATTRIBUTES = ['profileId', 'companyID', 'sex', 'nationality', 'frequentFlyer', 'isVip', 'bySelf', 'corporateTariffCode']\n",
    "UNNEEDED_ATTRIBUTES = [\n",
    "    'ranker_id', 'isAccess3D', 'totalPrice', 'taxes', 'legs0_arrivalAt', 'legs0_duration', 'frequent_flyer',\n",
    "    r'^legs1_(departureAt|arrivalAt|duration)$'\n",
    "    r'^legs[01]_segments[0-3]_(operatingCarrier_code|aircraft_code|flightNumber)$',\n",
    "    r'^legs[01]_segments[0-3]_(arrivalTo|baggage|seats).*$'\n",
    "]\n",
    "POLARS_INDEX_COL = ['__index_level_0__']\n",
    "MAJOR_HUBS = ['ATL','DXB','DFW','HND','LHR','DEN','ORD','IST','PVG','ICN','CDG', 'JFK','CLT','MEX','SFO','EWR','MIA','BKK','GRU','HKG']\n",
    "\n",
    "\n",
    "def get_cabin_class_columns(df: pl.DataFrame) -> List[str]:\n",
    "    \"\"\"Get all cabin class columns from the dataframe.\"\"\"\n",
    "    columns = df.columns\n",
    "    return [col for col in columns if col.startswith('legs') and col.endswith('_cabinClass')]\n",
    "\n",
    "\n",
    "def create_customer_aggregation_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create customer aggregation expressions for basic attributes and search behavior.\"\"\"\n",
    "    return [\n",
    "        # Basic customer attributes (take first non-null value per customer)\n",
    "        pl.col('companyID').drop_nulls().first().alias('companyID'),\n",
    "        pl.col('sex').drop_nulls().first().alias('sex'),\n",
    "        pl.col('nationality').drop_nulls().first().alias('nationality'),\n",
    "        pl.col('frequentFlyer').drop_nulls().first().alias('frequentFlyer'),\n",
    "        pl.col('isVip').drop_nulls().first().alias('isVip'),\n",
    "        pl.col('bySelf').drop_nulls().first().alias('bySelf'),\n",
    "        pl.col('corporateTariffCode').drop_nulls().first().alias('corporateTariffCode'),\n",
    "\n",
    "        # Normalized frequentFlyer program, addressing null values as null strings, and translating UT program\n",
    "        pl.col('frequentFlyer').drop_nulls().first().str.replace('- ЮТэйр ЗАО', 'UT').fill_null('').alias('ff_normalized'),\n",
    "\n",
    "        # Search behavior metrics\n",
    "        pl.len().alias('total_searches'),\n",
    "        pl.col('legs1_departureAt').is_not_null().mean().alias('roundtrip_preference'),\n",
    "        pl.col('searchRoute').drop_nulls().n_unique().alias('unique_routes_searched'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_booking_lead_time_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create booking lead time statistics.\"\"\"\n",
    "    # Calculate booking lead time in days\n",
    "    booking_lead_expr = (\n",
    "        (pl.col('legs0_departureAt').str.to_datetime() -\n",
    "         pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1)\n",
    "    ).cast(pl.Int32)\n",
    "\n",
    "    return [\n",
    "        booking_lead_expr.min().alias('min_booking_lead_days'),\n",
    "        booking_lead_expr.max().alias('max_booking_lead_days'),\n",
    "        booking_lead_expr.mean().alias('avg_booking_lead_days'),\n",
    "        booking_lead_expr.median().alias('median_booking_lead_days'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_travel_preference_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create travel preference features for most common airports and carriers.\"\"\"\n",
    "    return [\n",
    "        # Most common departure airport\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().mode().first().alias('most_common_departure_airport'),\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().n_unique().alias('unique_departure_airports'),\n",
    "\n",
    "        # Most common marketing carrier\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').drop_nulls().mode().first().alias('most_common_carrier'),\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').drop_nulls().n_unique().alias('unique_carriers_used'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_cabin_class_features(cabin_class_cols: List[str]) -> List[pl.Expr]:\n",
    "    \"\"\"Create cabin class preference statistics.\"\"\"\n",
    "    if not cabin_class_cols:\n",
    "        # Return default values if no cabin class columns found\n",
    "        return [\n",
    "            pl.lit(None).alias('min_cabin_class'),\n",
    "            pl.lit(None).alias('max_cabin_class'),\n",
    "            pl.lit(None).alias('avg_cabin_class'),\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        # Cabin class statistics across all segments\n",
    "        pl.min_horizontal([pl.col(col) for col in cabin_class_cols]).min().alias('min_cabin_class'),\n",
    "        pl.max_horizontal([pl.col(col) for col in cabin_class_cols]).max().alias('max_cabin_class'),\n",
    "        pl.mean_horizontal([pl.col(col) for col in cabin_class_cols]).mean().alias('avg_cabin_class'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_temporal_preference_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create temporal preference features for departure patterns.\"\"\"\n",
    "    return [\n",
    "        # Weekday preference (most common day of week for departures)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.weekday()\n",
    "          .mode().first().alias('weekday_preference'),\n",
    "\n",
    "        # Weekend travel rate (percentage of weekend departures - 5=Sat, 6=Sun)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.weekday()\n",
    "          .map_elements(lambda x: 1 if x >= 5 else 0, return_dtype=pl.Int8)\n",
    "          .mean().alias('weekend_travel_rate'),\n",
    "\n",
    "        # Time of day variance (how consistent are their departure times)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.hour()\n",
    "          .std().alias('time_of_day_variance'),\n",
    "\n",
    "        # Night flight preference (flights departing 22:00-06:00)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.hour()\n",
    "          .map_elements(lambda x: 1 if (x >= 22 or x < 6) else 0, return_dtype=pl.Int8)\n",
    "          .mean().alias('night_flight_preference')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_route_specific_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create features related to route preferences and characteristics.\"\"\"\n",
    "\n",
    "    return [\n",
    "        # Route loyalty (how frequently they search the same routes)\n",
    "        (pl.col('searchRoute').n_unique() / pl.len())\n",
    "          .map_elements(lambda x: 1 - x if x > 0 else 0)  # Invert so higher = more loyal\n",
    "          .alias('route_loyalty'),\n",
    "\n",
    "        # Hub preference (preference for major hub airports)\n",
    "        pl.concat_list([\n",
    "            pl.col('legs0_segments0_departureFrom_airport_iata').is_in(MAJOR_HUBS),\n",
    "            pl.col('legs0_segments0_arrivalTo_airport_iata').is_in(MAJOR_HUBS)\n",
    "        ]).list.mean().alias('hub_preference'),\n",
    "\n",
    "        # Connection tolerance (preference for flights with connections)\n",
    "        pl.col('total_segments').mean().alias('connection_tolerance'),\n",
    "\n",
    "        # Short haul preference\n",
    "        (1 - (pl.col('legs0_duration').str.extract(r'^(\\d+):(\\d+)', 1).cast(pl.Int32) / 12))\n",
    "          .clip(0, 1).alias('short_haul_preference'),\n",
    "\n",
    "        # Domestic/international ratio based on route length\n",
    "        # Assuming routes with same first letter in IATA codes are likely domestic\n",
    "        pl.col('searchRoute').map_elements(\n",
    "            lambda route: 1 if route and route[:1] == route[3:4] else 0,\n",
    "            return_dtype=pl.Int8\n",
    "        ).mean().alias('domestic_international_ratio')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_price_sensitivity_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create features related to price sensitivity and patterns.\"\"\"\n",
    "    return [\n",
    "        # Price position preference (typical percentile chosen)\n",
    "        pl.col('price_percentile').mean().alias('price_position_preference'),\n",
    "\n",
    "        # Price to duration sensitivity\n",
    "        # Higher values mean more willing to pay for shorter flights\n",
    "        pl.covar(\n",
    "            pl.col('totalPrice'),\n",
    "            pl.col('total_duration') * -1  # Negative so higher = more sensitive\n",
    "        ).alias('price_to_duration_sensitivity'),\n",
    "\n",
    "        # Premium economy preference (assuming cabin class 2 is premium economy)\n",
    "        pl.mean_horizontal([\n",
    "            pl.col(f'legs0_segments{i}_cabinClass') == 2\n",
    "            for i in range(4)\n",
    "        ]).mean().alias('premium_economy_preference'),\n",
    "\n",
    "        # Consistent price tier (lower variance = more consistent)\n",
    "        pl.col('price_tier').std().map_elements(\n",
    "            lambda x: 1 - min(x / 3, 1) if x is not None else 0.5  # Invert and normalize\n",
    "        ).alias('consistent_price_tier')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_service_preference_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create features related to service preferences.\"\"\"\n",
    "    return [\n",
    "        # Baggage preference (average selected baggage allowance)\n",
    "        pl.concat_list([\n",
    "            pl.col(f'legs0_segments{i}_baggageAllowance_quantity')\n",
    "            for i in range(4)\n",
    "        ]).list.mean().alias('baggage_preference'),\n",
    "\n",
    "        # Loyalty program utilization\n",
    "        pl.col('frequentFlyer').is_not_null().mean().alias('loyalty_program_utilization')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_derived_metrics() -> List[pl.Expr]:\n",
    "    \"\"\"Create complex derived metrics from combinations of features.\"\"\"\n",
    "    return [\n",
    "        # Price flexibility index (higher price variance / booking rate = more flexible)\n",
    "        (pl.col('totalPrice').std() /\n",
    "         pl.col('booking_rate').clip(0.01, 1))\n",
    "        .alias('price_flexibility_index'),\n",
    "\n",
    "        # Convenience priority score (higher = more emphasis on convenient times)\n",
    "        ((1 - pl.col('time_of_day_variance')) * 10 +\n",
    "         pl.col('price_to_duration_sensitivity') * 5)\n",
    "        .alias('convenience_priority_score'),\n",
    "\n",
    "        # Loyalty vs price index (higher = more loyal, less price sensitive)\n",
    "        (pl.col('loyalty_program_utilization') * 10 -\n",
    "         pl.col('price_position_preference') / 10)\n",
    "        .alias('loyalty_vs_price_index'),\n",
    "\n",
    "        # Planning consistency score (inverse of lead time variance)\n",
    "        (1 / (pl.col('max_booking_lead_days') - pl.col('min_booking_lead_days') + 1))\n",
    "        .alias('planning_consistency_score'),\n",
    "\n",
    "        # Luxury index (combination of cabin class and price tier)\n",
    "        (pl.col('avg_cabin_class') * 20 +\n",
    "         pl.col('price_position_preference') / 2)\n",
    "        .alias('luxury_index')\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_customer_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract customer features for clustering analysis.\n",
    "    Aggregates by profileId to create customer-level features.\n",
    "    \"\"\"\n",
    "    # Check if already processed\n",
    "    if df.height > 0 and 'total_searches' in df.columns:\n",
    "        return df\n",
    "\n",
    "    # Get cabin class columns\n",
    "    cabin_class_cols = [col for col in df.columns if col.startswith('legs') and col.endswith('_cabinClass')]\n",
    "\n",
    "    # Create lazy frame and group by profileId\n",
    "    lazy_df = df.lazy().group_by('profileId')\n",
    "\n",
    "    # Apply feature groups\n",
    "    customer_features = lazy_df.agg([\n",
    "        *create_customer_aggregation_features(),\n",
    "        *create_booking_lead_time_features(),\n",
    "        *create_travel_preference_features(),\n",
    "        *create_cabin_class_features(cabin_class_cols),\n",
    "        *create_temporal_preference_features(),\n",
    "        *create_route_specific_features(),\n",
    "        *create_price_sensitivity_features(),\n",
    "        *create_service_preference_features()\n",
    "    ])\n",
    "\n",
    "    # Materialize to generate the basic features\n",
    "    base_features = customer_features.collect()\n",
    "\n",
    "    # Add the derived metrics that depend on the generated features\n",
    "    enhanced_features = base_features.with_columns(create_derived_metrics())\n",
    "\n",
    "    print(f\"Generated {len(enhanced_features.columns)} customer features for {len(enhanced_features)} customers\")\n",
    "    return enhanced_features\n"
   ],
   "id": "7fcf275e75e2d6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "class ScalableFlightRecommendationModel:\n",
    "    \"\"\"\n",
    "    Memory-efficient flight recommendation model designed for large datasets (10M+ rows).\n",
    "    Uses streaming processing, chunked operations, and optimized data structures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_customer_segments=50, chunk_size=100000, random_state=42):\n",
    "        self.n_customer_segments = n_customer_segments\n",
    "        self.chunk_size = chunk_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Models\n",
    "        self.customer_segmentation_model = None\n",
    "        self.global_model = None\n",
    "\n",
    "        # Preprocessing\n",
    "        self.customer_scaler = StandardScaler()\n",
    "        self.segment_centroids = None\n",
    "\n",
    "        # Feature lists\n",
    "        self.customer_features = []\n",
    "        self.flight_features = []\n",
    "\n",
    "    def create_flight_features_batch(self, df_chunk: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create flight-level features\n",
    "        \"\"\"\n",
    "\n",
    "        new_columns = []\n",
    "\n",
    "        # Price features (always relative to search session)\n",
    "        new_columns.extend([\n",
    "            (pl.col('totalPrice').rank(method='ordinal').over('ranker_id') /\n",
    "             pl.col('totalPrice').count().over('ranker_id')).alias('price_rank_pct'),\n",
    "            (pl.col('totalPrice') / pl.col('totalPrice').min().over('ranker_id')).alias('price_ratio_to_min'),\n",
    "        ])\n",
    "\n",
    "        # Duration features - only create if not already present\n",
    "        if 'duration_hours' not in df_chunk.columns and 'legs0_duration' in df_chunk.columns:\n",
    "            new_columns.extend([\n",
    "                pl.col('legs0_duration').str.split(':').list.get(0).cast(pl.Int32, strict=False).alias('duration_hours'),\n",
    "                pl.col('legs0_duration').str.split(':').list.get(1).cast(pl.Int32, strict=False).alias('duration_minutes'),\n",
    "            ])\n",
    "\n",
    "        # Total segments - only if not present\n",
    "        if 'total_segments' not in df_chunk.columns:\n",
    "            new_columns.append(\n",
    "                sum([pl.col(f'legs0_segments{i}_departureFrom_airport_iata').is_not_null().cast(pl.Int8)\n",
    "                     for i in range(4)]).alias('total_segments')\n",
    "            )\n",
    "\n",
    "        # Temporal features - only if not present\n",
    "        if 'departure_hour' not in df_chunk.columns and 'legs0_departureAt' in df_chunk.columns:\n",
    "            new_columns.extend([\n",
    "                pl.col('legs0_departureAt').str.to_datetime().dt.hour().alias('departure_hour'),\n",
    "                pl.col('legs0_departureAt').str.to_datetime().dt.weekday().alias('departure_weekday'),\n",
    "            ])\n",
    "\n",
    "        # Lead time - only if not present\n",
    "        if 'booking_lead_days' not in df_chunk.columns and all(col in df_chunk.columns for col in ['legs0_departureAt', 'requestDate']):\n",
    "            new_columns.append(\n",
    "                ((pl.col('legs0_departureAt').str.to_datetime() -\n",
    "                  pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1)).cast(pl.Int32).alias('booking_lead_days')\n",
    "            )\n",
    "\n",
    "        # Primary carrier\n",
    "        if 'primary_carrier' not in df_chunk.columns and 'legs0_segments0_marketingCarrier_code' in df_chunk.columns:\n",
    "            new_columns.append(\n",
    "                pl.col('legs0_segments0_marketingCarrier_code').fill_null('unknown').alias('primary_carrier')\n",
    "            )\n",
    "\n",
    "        # Apply new columns if any\n",
    "        if new_columns:\n",
    "            df_chunk = df_chunk.with_columns(new_columns)\n",
    "\n",
    "        # Derived features - create based on what's available\n",
    "        derived_columns = []\n",
    "\n",
    "        if 'departure_hour' in df_chunk.columns:\n",
    "            if 'is_daytime' not in df_chunk.columns:\n",
    "                derived_columns.append((pl.col('departure_hour').is_between(6, 22)).cast(pl.Int8).alias('is_daytime'))\n",
    "\n",
    "        if 'departure_weekday' in df_chunk.columns:\n",
    "            if 'is_weekend' not in df_chunk.columns:\n",
    "                derived_columns.append((pl.col('departure_weekday') >= 5).cast(pl.Int8).alias('is_weekend'))\n",
    "\n",
    "        if 'total_segments' in df_chunk.columns:\n",
    "            if 'has_connections' not in df_chunk.columns:\n",
    "                derived_columns.append((pl.col('total_segments') > 1).cast(pl.Int8).alias('has_connections'))\n",
    "\n",
    "        if all(col in df_chunk.columns for col in ['duration_hours', 'duration_minutes']):\n",
    "            if 'total_duration_mins' not in df_chunk.columns:\n",
    "                derived_columns.append(\n",
    "                    (pl.col('duration_hours').fill_null(0) * 60 + pl.col('duration_minutes').fill_null(0)).alias('total_duration_mins')\n",
    "                )\n",
    "\n",
    "        if derived_columns:\n",
    "            df_chunk = df_chunk.with_columns(derived_columns)\n",
    "\n",
    "        return df_chunk\n",
    "\n",
    "    def create_customer_segments_streaming(self, customer_features_df: pl.DataFrame) -> Dict:\n",
    "        \"\"\"Create customer segments using Agglomerative Clustering based on evaluation results.\"\"\"\n",
    "\n",
    "        # Core behavioral features for segmentation based on your successful evaluation\n",
    "        segmentation_features = [\n",
    "            'total_searches', 'isVip', 'roundtrip_preference',\n",
    "            'avg_booking_lead_days', 'unique_carriers_used',\n",
    "            # Additional features that likely contributed to good clustering\n",
    "            'weekend_travel_rate', 'route_loyalty', 'hub_preference', 'connection_tolerance'\n",
    "        ]\n",
    "\n",
    "        # Filter available features\n",
    "        available_features = [f for f in segmentation_features if f in customer_features_df.columns]\n",
    "        print(f\"Using {len(available_features)} features for Agglomerative clustering: {available_features}\")\n",
    "\n",
    "        # Convert to numpy for sklearn\n",
    "        X_segment = customer_features_df.select(available_features).to_numpy()\n",
    "        X_segment = np.nan_to_num(X_segment, nan=0.0)\n",
    "\n",
    "        # Scale features\n",
    "        X_segment_scaled = self.customer_scaler.fit_transform(X_segment)\n",
    "\n",
    "        # Use Agglomerative Clustering based on your evaluation results\n",
    "        print(f\"Clustering {len(X_segment)} customers using Agglomerative Clustering...\")\n",
    "\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "        # Create connectivity matrix for more efficient clustering on large datasets\n",
    "        if len(X_segment) > 10000:\n",
    "            print(\"Creating connectivity graph for large dataset...\")\n",
    "            connectivity = kneighbors_graph(\n",
    "                X_segment_scaled, n_neighbors=10, include_self=False\n",
    "            )\n",
    "        else:\n",
    "            connectivity = None\n",
    "\n",
    "        # Use optimal number of clusters from your evaluation (10)\n",
    "        # But allow for scaling with larger datasets\n",
    "        n_clusters = min(10, max(5, len(X_segment) // 1000))  # Scale clusters with data size\n",
    "\n",
    "        self.customer_segmentation_model = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters,\n",
    "            connectivity=connectivity,\n",
    "            linkage='ward'  # Generally works well with scaled features\n",
    "        )\n",
    "\n",
    "        segments = self.customer_segmentation_model.fit_predict(X_segment_scaled)\n",
    "\n",
    "        # Create pseudo-centroids for consistency with prediction pipeline\n",
    "        # (Agglomerative doesn't have centroids, so we compute them)\n",
    "        unique_segments = np.unique(segments)\n",
    "        centroids = []\n",
    "        for segment_id in unique_segments:\n",
    "            segment_mask = segments == segment_id\n",
    "            centroid = X_segment_scaled[segment_mask].mean(axis=0)\n",
    "            centroids.append(centroid)\n",
    "\n",
    "        centroids = np.array(centroids)\n",
    "\n",
    "        # Analyze cluster profiles like in your evaluation\n",
    "        customer_df = customer_features_df.to_pandas()\n",
    "        customer_df['cluster'] = segments\n",
    "\n",
    "        print(\"\\n📊 CLUSTER PROFILES:\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        cluster_profiles = customer_df.groupby('cluster').agg({\n",
    "            'total_searches': 'mean',\n",
    "            'isVip': 'mean',  # This gives VIP rate\n",
    "            'roundtrip_preference': 'mean',\n",
    "            'avg_booking_lead_days': 'mean',\n",
    "            'unique_carriers_used': 'mean'\n",
    "        }).round(2)\n",
    "\n",
    "        cluster_sizes = customer_df.groupby('cluster').size()\n",
    "        cluster_profiles['size'] = cluster_sizes\n",
    "\n",
    "        print(cluster_profiles)\n",
    "\n",
    "        # Store segment info\n",
    "        segment_info = {\n",
    "            'segments': segments,\n",
    "            'centroids': centroids,\n",
    "            'feature_names': available_features,\n",
    "            'cluster_profiles': cluster_profiles,\n",
    "            'n_clusters': n_clusters\n",
    "        }\n",
    "\n",
    "        return segment_info\n",
    "\n",
    "    def prepare_training_data_chunked(self, df: pl.DataFrame, customer_features_df: pl.DataFrame,\n",
    "                                    segment_info: Dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare training data in chunks to manage memory.\"\"\"\n",
    "\n",
    "        print(\"Adding segments to customer features...\")\n",
    "        customer_with_segments = customer_features_df.with_columns([\n",
    "            pl.Series('customer_segment', segment_info['segments'])\n",
    "        ])\n",
    "\n",
    "        # Define feature columns ONCE before chunking\n",
    "        feature_cols = [\n",
    "            # Customer features (select key ones to reduce dimensionality)\n",
    "            'total_searches', 'roundtrip_preference', 'avg_booking_lead_days',\n",
    "            'weekend_travel_rate', 'route_loyalty', 'customer_segment',\n",
    "\n",
    "            # Flight features\n",
    "            'price_rank_pct', 'price_ratio_to_min', 'duration_hours', 'total_segments',\n",
    "            'departure_hour', 'booking_lead_days', 'total_duration_mins',\n",
    "            'is_daytime', 'is_weekend', 'has_connections',\n",
    "\n",
    "            # Interactions\n",
    "            'daytime_alignment', 'weekend_alignment', 'price_preference_match', 'carrier_loyalty_match'\n",
    "        ]\n",
    "\n",
    "        # Test with a small sample to determine available columns\n",
    "        print(\"Determining available features from data sample...\")\n",
    "        sample_ranker_ids = df.select('ranker_id').unique().limit(10)['ranker_id'].to_list()\n",
    "        df_sample = df.filter(pl.col('ranker_id').is_in(sample_ranker_ids))\n",
    "\n",
    "        # Process sample to see what columns will be available\n",
    "        df_sample = self.create_flight_features_batch(df_sample)\n",
    "        df_sample = df_sample.join(customer_with_segments, on='profileId', how='left')\n",
    "        df_sample = df_sample.with_columns([\n",
    "            (pl.col('is_daytime') * (1.0 - pl.col('night_flight_preference').fill_null(0.5))).alias('daytime_alignment'),\n",
    "            (pl.col('is_weekend') * pl.col('weekend_travel_rate').fill_null(0.5)).alias('weekend_alignment'),\n",
    "            (pl.col('price_rank_pct') * pl.col('price_position_preference').fill_null(0.5)).alias('price_preference_match'),\n",
    "            (pl.col('primary_carrier') == pl.col('most_common_carrier').fill_null('unknown')).cast(pl.Int8).alias('carrier_loyalty_match'),\n",
    "        ])\n",
    "\n",
    "        # Determine final feature list ONCE\n",
    "        available_cols = [col for col in feature_cols if col in df_sample.columns]\n",
    "        self.flight_features = available_cols  # Set this ONCE before chunking\n",
    "\n",
    "        print(f\"Will use {len(available_cols)} features: {available_cols}\")\n",
    "        del df_sample  # Clean up sample\n",
    "\n",
    "        # Process data in chunks\n",
    "        print(f\"Processing {len(df)} rows in chunks of {self.chunk_size}...\")\n",
    "\n",
    "        X_chunks = []\n",
    "        y_chunks = []\n",
    "        group_chunks = []\n",
    "\n",
    "        # Get unique ranker_ids to avoid splitting groups\n",
    "        ranker_ids = df.select('ranker_id').unique().sort('ranker_id')['ranker_id'].to_list()\n",
    "\n",
    "        for i in range(0, len(ranker_ids), self.chunk_size):\n",
    "            chunk_ranker_ids = ranker_ids[i:i + self.chunk_size]\n",
    "\n",
    "            # Filter chunk by ranker_ids\n",
    "            df_chunk = df.filter(pl.col('ranker_id').is_in(chunk_ranker_ids))\n",
    "\n",
    "            print(f\"Processing chunk {i//self.chunk_size + 1}: {len(df_chunk)} rows\")\n",
    "\n",
    "            # Add flight features\n",
    "            df_chunk = self.create_flight_features_batch(df_chunk)\n",
    "\n",
    "            # Join with customer features\n",
    "            df_chunk = df_chunk.join(customer_with_segments, on='profileId', how='left')\n",
    "\n",
    "            # Create interaction features efficiently\n",
    "            df_chunk = df_chunk.with_columns([\n",
    "                (pl.col('is_daytime') * (1.0 - pl.col('night_flight_preference').fill_null(0.5))).alias('daytime_alignment'),\n",
    "                (pl.col('is_weekend') * pl.col('weekend_travel_rate').fill_null(0.5)).alias('weekend_alignment'),\n",
    "                (pl.col('price_rank_pct') * pl.col('price_position_preference').fill_null(0.5)).alias('price_preference_match'),\n",
    "                (pl.col('primary_carrier') == pl.col('most_common_carrier').fill_null('unknown')).cast(pl.Int8).alias('carrier_loyalty_match'),\n",
    "            ])\n",
    "\n",
    "            # Use the predetermined feature list (consistent across all chunks)\n",
    "            try:\n",
    "                X_chunk = df_chunk.select(self.flight_features).fill_null(0).to_numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting features in chunk {i//self.chunk_size + 1}: {e}\")\n",
    "                # Fallback: check what columns are actually available in this chunk\n",
    "                chunk_available_cols = [col for col in self.flight_features if col in df_chunk.columns]\n",
    "                print(f\"Available columns in this chunk: {chunk_available_cols}\")\n",
    "                # Pad missing columns with zeros\n",
    "                chunk_data = df_chunk.select(chunk_available_cols).fill_null(0).to_pandas()\n",
    "                for missing_col in [col for col in self.flight_features if col not in chunk_available_cols]:\n",
    "                    chunk_data[missing_col] = 0\n",
    "                # Reorder to match self.flight_features order\n",
    "                X_chunk = chunk_data[self.flight_features].to_numpy()\n",
    "\n",
    "            y_chunk = df_chunk.select('selected').to_numpy().ravel()\n",
    "            group_chunk = df_chunk.select('ranker_id').to_numpy().ravel()\n",
    "\n",
    "            X_chunks.append(X_chunk)\n",
    "            y_chunks.append(y_chunk)\n",
    "            group_chunks.append(group_chunk)\n",
    "\n",
    "            # Clean up memory\n",
    "            del df_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        # Combine chunks\n",
    "        print(\"Combining chunks...\")\n",
    "        X = np.vstack(X_chunks)\n",
    "        y = np.concatenate(y_chunks)\n",
    "        groups = np.concatenate(group_chunks)\n",
    "\n",
    "        print(f\"Final training data: {X.shape[0]} rows, {X.shape[1]} features\")\n",
    "        print(f\"Feature consistency check: Expected {len(self.flight_features)} features, got {X.shape[1]}\")\n",
    "\n",
    "        return X, y, groups\n",
    "\n",
    "    def fit(self, df: pl.DataFrame, customer_features_df: pl.DataFrame) -> 'ScalableFlightRecommendationModel':\n",
    "        \"\"\"Fit the model using memory-efficient processing.\"\"\"\n",
    "\n",
    "        print(\"Creating customer segments...\")\n",
    "        segment_info = self.create_customer_segments_streaming(customer_features_df)  # Agglomerative Clustering\n",
    "\n",
    "        # Store centroids for prediction (needed for Agglomerative Clustering)\n",
    "        self.segment_centroids = segment_info['centroids']\n",
    "\n",
    "        print(\"Preparing training data...\")\n",
    "        X, y, groups = self.prepare_training_data_chunked(df, customer_features_df, segment_info)\n",
    "\n",
    "        print(\"Training LightGBM model...\")\n",
    "\n",
    "        # Create group sizes for LightGBM ranking\n",
    "        unique_groups, group_sizes = np.unique(groups, return_counts=True)\n",
    "\n",
    "        # LightGBM with memory-efficient settings\n",
    "        self.global_model = lgb.LGBMRanker(\n",
    "            objective='lambdarank',\n",
    "            metric='ndcg',\n",
    "            boosting_type='gbdt',\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.1,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            bagging_freq=5,\n",
    "            verbose=-1,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=4,\n",
    "            force_row_wise=True  # Memory efficient\n",
    "        )\n",
    "\n",
    "        self.global_model.fit(\n",
    "            X, y,\n",
    "            group=group_sizes,\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "        )\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        print(f\"Final model uses {len(self.flight_features)} features\")\n",
    "        print(f\"Customer segments: {segment_info['n_clusters']}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba_chunked(self, df: pl.DataFrame, customer_features_df: pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Make predictions in chunks to manage memory.\"\"\"\n",
    "\n",
    "        print(\"Predicting customer segments...\")\n",
    "        # Get segmentation features\n",
    "        segmentation_features = [f for f in ['total_searches', 'isVip', 'roundtrip_preference',\n",
    "                                           'avg_booking_lead_days', 'unique_carriers_used',\n",
    "                                           'weekend_travel_rate', 'route_loyalty', 'hub_preference', 'connection_tolerance']\n",
    "                               if f in customer_features_df.columns]\n",
    "\n",
    "        X_segment = customer_features_df.select(segmentation_features).to_numpy()\n",
    "        X_segment = np.nan_to_num(X_segment, nan=0.0)\n",
    "        X_segment_scaled = self.customer_scaler.transform(X_segment)\n",
    "\n",
    "        # For Agglomerative Clustering, we need to predict segments by finding closest centroids\n",
    "        # (since AgglomerativeClustering doesn't have a predict method)\n",
    "        if hasattr(self.customer_segmentation_model, 'cluster_centers_'):\n",
    "            # This is KMeans-based\n",
    "            segments = self.customer_segmentation_model.predict(X_segment_scaled)\n",
    "        else:\n",
    "            # This is Agglomerative - find closest centroid for each customer\n",
    "            from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "            # Get centroids from segment_info (stored during training)\n",
    "            if not hasattr(self, 'segment_centroids'):\n",
    "                print(\"Warning: No segment centroids found. Using default assignment.\")\n",
    "                segments = np.zeros(len(X_segment_scaled), dtype=int)\n",
    "            else:\n",
    "                distances = euclidean_distances(X_segment_scaled, self.segment_centroids)\n",
    "                segments = np.argmin(distances, axis=1)\n",
    "\n",
    "        customer_with_segments = customer_features_df.with_columns([\n",
    "            pl.Series('customer_segment', segments)\n",
    "        ])\n",
    "\n",
    "        print(f\"Making predictions for {len(df)} rows using {len(self.flight_features)} features...\")\n",
    "        print(f\"Expected features: {self.flight_features}\")\n",
    "\n",
    "        # Get unique ranker_ids for chunking\n",
    "        ranker_ids = df.select('ranker_id').unique().sort('ranker_id')['ranker_id'].to_list()\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0, len(ranker_ids), self.chunk_size):\n",
    "            chunk_ranker_ids = ranker_ids[i:i + self.chunk_size]\n",
    "\n",
    "            # Process chunk\n",
    "            df_chunk = df.filter(pl.col('ranker_id').is_in(chunk_ranker_ids))\n",
    "            df_chunk = self.create_flight_features_batch(df_chunk)\n",
    "            df_chunk = df_chunk.join(customer_with_segments, on='profileId', how='left')\n",
    "\n",
    "            # Add interaction features\n",
    "            df_chunk = df_chunk.with_columns([\n",
    "                (pl.col('is_daytime') * (1.0 - pl.col('night_flight_preference').fill_null(0.5))).alias('daytime_alignment'),\n",
    "                (pl.col('is_weekend') * pl.col('weekend_travel_rate').fill_null(0.5)).alias('weekend_alignment'),\n",
    "                (pl.col('price_rank_pct') * pl.col('price_position_preference').fill_null(0.5)).alias('price_preference_match'),\n",
    "                (pl.col('primary_carrier') == pl.col('most_common_carrier').fill_null('unknown')).cast(pl.Int8).alias('carrier_loyalty_match'),\n",
    "            ])\n",
    "\n",
    "            # Extract features using the SAME feature list from training\n",
    "            try:\n",
    "                X_chunk = df_chunk.select(self.flight_features).fill_null(0).to_numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting features in prediction chunk {i//self.chunk_size + 1}: {e}\")\n",
    "                # Check what's available vs what's expected\n",
    "                available_in_chunk = [col for col in self.flight_features if col in df_chunk.columns]\n",
    "                missing_in_chunk = [col for col in self.flight_features if col not in df_chunk.columns]\n",
    "\n",
    "                print(f\"Available: {available_in_chunk}\")\n",
    "                print(f\"Missing: {missing_in_chunk}\")\n",
    "\n",
    "                # Create dataframe with available columns and pad missing ones with zeros\n",
    "                chunk_data = df_chunk.select(available_in_chunk).fill_null(0).to_pandas()\n",
    "                for missing_col in missing_in_chunk:\n",
    "                    chunk_data[missing_col] = 0.0\n",
    "\n",
    "                # Ensure same order as training\n",
    "                X_chunk = chunk_data[self.flight_features].to_numpy()\n",
    "\n",
    "            # Predict\n",
    "            chunk_predictions = self.global_model.predict(X_chunk)\n",
    "            predictions.append(chunk_predictions)\n",
    "\n",
    "            print(f\"Processed chunk {i//self.chunk_size + 1}/{len(range(0, len(ranker_ids), self.chunk_size))}\")\n",
    "\n",
    "            del df_chunk, X_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        final_predictions = np.concatenate(predictions)\n",
    "        print(f\"Generated {len(final_predictions)} predictions\")\n",
    "        return final_predictions\n",
    "\n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save model efficiently.\"\"\"\n",
    "        model_data = {\n",
    "            'customer_segmentation_model': self.customer_segmentation_model,\n",
    "            'global_model': self.global_model,\n",
    "            'customer_scaler': self.customer_scaler,\n",
    "            'flight_features': self.flight_features,\n",
    "            'n_customer_segments': self.n_customer_segments,\n",
    "            'chunk_size': self.chunk_size,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        joblib.dump(model_data, filepath, compress=3)\n",
    "\n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"Load saved model.\"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        for key, value in model_data.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "# Memory-efficient training function\n",
    "def train_scalable_model(train_df: pl.DataFrame, test_df: pl.DataFrame = None) -> Tuple[np.ndarray, ScalableFlightRecommendationModel]:\n",
    "    \"\"\"\n",
    "    Train model efficiently on large dataset.\n",
    "\n",
    "    Estimated memory usage: ~4-6GB for 18M rows\n",
    "    Training time: ~30-60 minutes on modern hardware\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Extracting customer features...\")\n",
    "    train_customer_features = extract_customer_features(train_df)\n",
    "\n",
    "    print(\"Initializing scalable model...\")\n",
    "    model = ScalableFlightRecommendationModel(\n",
    "        n_customer_segments=50,  # Increased for better granularity\n",
    "        chunk_size=50000  # Adjust based on available RAM\n",
    "    )\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model.fit(train_df, train_customer_features)\n",
    "\n",
    "    predictions = None\n",
    "    if test_df is not None:\n",
    "        print(\"Making predictions...\")\n",
    "        test_customer_features = extract_customer_features(test_df)\n",
    "        predictions = model.predict_proba_chunked(test_df, test_customer_features)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    model.save_model('scalable_flight_model.joblib')\n",
    "\n",
    "    return predictions, model\n",
    "\n",
    "\n",
    "# Performance monitoring utilities\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor memory and time usage during training.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def estimate_memory_usage(n_rows: int, n_features: int) -> str:\n",
    "        \"\"\"Estimate memory usage.\"\"\"\n",
    "        # Rough estimates in GB\n",
    "        base_data = (n_rows * n_features * 8) / (1024**3)  # float64\n",
    "        working_memory = base_data * 2  # intermediate calculations\n",
    "        model_memory = 0.5  # model storage\n",
    "\n",
    "        total = base_data + working_memory + model_memory\n",
    "        return f\"Estimated peak memory usage: {total:.1f}GB\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_recommendations(n_rows: int) -> Dict[str, any]:\n",
    "        \"\"\"Get performance recommendations based on data size.\"\"\"\n",
    "\n",
    "        if n_rows > 50_000_000:\n",
    "            return {\n",
    "                'chunk_size': 25000,\n",
    "                'n_segments': 100,\n",
    "                'early_stopping': 30,\n",
    "                'feature_fraction': 0.6,\n",
    "                'recommendation': 'Consider using a cluster or high-memory machine (32GB+ RAM)'\n",
    "            }\n",
    "        elif n_rows > 10_000_000:\n",
    "            return {\n",
    "                'chunk_size': 50000,\n",
    "                'n_segments': 50,\n",
    "                'early_stopping': 50,\n",
    "                'feature_fraction': 0.8,\n",
    "                'recommendation': 'Should work well on 16GB+ RAM machine'\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'chunk_size': 100000,\n",
    "                'n_segments': 25,\n",
    "                'early_stopping': 100,\n",
    "                'feature_fraction': 0.9,\n",
    "                'recommendation': 'Standard configuration should work fine'\n",
    "            }\n",
    "\n",
    "# Optimized customer feature extraction maintaining lazy evaluation\n",
    "def extract_customer_features_scalable(df: pl.DataFrame, chunk_size: int = 100000) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Memory-efficient version of customer feature extraction.\n",
    "    Maintains lazy evaluation for optimal performance.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting customer features from {len(df)} rows using lazy evaluation...\")\n",
    "\n",
    "    # Get cabin class columns that exist\n",
    "    cabin_class_cols = [col for col in df.columns if col.startswith('legs') and col.endswith('_cabinClass')]\n",
    "\n",
    "    # Core aggregations - optimized for memory efficiency\n",
    "    customer_aggs = [\n",
    "        # Basic attributes (first non-null)\n",
    "        pl.col('companyID').drop_nulls().first().alias('companyID'),\n",
    "        pl.col('sex').drop_nulls().first().alias('sex'),\n",
    "        pl.col('nationality').drop_nulls().first().alias('nationality'),\n",
    "        pl.col('frequentFlyer').drop_nulls().first().alias('frequentFlyer'),\n",
    "        pl.col('isVip').drop_nulls().first().alias('isVip'),\n",
    "        pl.col('bySelf').drop_nulls().first().alias('bySelf'),\n",
    "        pl.col('corporateTariffCode').drop_nulls().first().alias('corporateTariffCode'),\n",
    "\n",
    "        # Normalized frequent flyer\n",
    "        pl.col('frequentFlyer').drop_nulls().first().str.replace('- ЮТэйр ЗАО', 'UT').fill_null('').alias('ff_normalized'),\n",
    "\n",
    "        # Search behavior (efficient counting)\n",
    "        pl.len().alias('total_searches'),\n",
    "        pl.col('legs1_departureAt').is_not_null().mean().alias('roundtrip_preference'),\n",
    "        pl.col('searchRoute').drop_nulls().n_unique().alias('unique_routes_searched'),\n",
    "\n",
    "        # Booking lead time (simplified calculation)\n",
    "        ((pl.col('legs0_departureAt').str.to_datetime() - pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1))\n",
    "        .cast(pl.Int32).mean().alias('avg_booking_lead_days'),\n",
    "\n",
    "        # Travel preferences (most common values)\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().mode().first().alias('most_common_departure_airport'),\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().n_unique().alias('unique_departure_airports'),\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').drop_nulls().mode().first().alias('most_common_carrier'),\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').drop_nulls().n_unique().alias('unique_carriers_used'),\n",
    "\n",
    "        # Temporal preferences\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.weekday().mode().first().alias('weekday_preference'),\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.weekday().map_elements(\n",
    "            lambda x: 1 if x >= 5 else 0, return_dtype=pl.Int8\n",
    "        ).mean().alias('weekend_travel_rate'),\n",
    "\n",
    "        # Night flight preference\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.hour().map_elements(\n",
    "            lambda x: 1 if (x >= 22 or x < 6) else 0, return_dtype=pl.Int8\n",
    "        ).mean().alias('night_flight_preference'),\n",
    "\n",
    "        # Route loyalty (simplified)\n",
    "        (1 - (pl.col('searchRoute').n_unique() / pl.len().clip(1, None))).alias('route_loyalty'),\n",
    "\n",
    "        # Hub preference\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').is_in([\n",
    "            'ATL','DXB','DFW','HND','LHR','DEN','ORD','IST','PVG','ICN','CDG','JFK','CLT','MEX','SFO','EWR','MIA','BKK','GRU','HKG'\n",
    "        ]).mean().alias('hub_preference'),\n",
    "\n",
    "        # Connection tolerance (average segments)\n",
    "        sum([pl.col(f'legs0_segments{i}_departureFrom_airport_iata').is_not_null().cast(pl.Int8) for i in range(4)])\n",
    "        .mean().alias('connection_tolerance'),\n",
    "    ]\n",
    "\n",
    "    # Add cabin class features if available\n",
    "    if cabin_class_cols:\n",
    "        cabin_aggs = [\n",
    "            pl.min_horizontal([pl.col(col) for col in cabin_class_cols]).mean().alias('avg_cabin_class'),\n",
    "        ]\n",
    "        customer_aggs.extend(cabin_aggs)\n",
    "\n",
    "    # Add price-related features if totalPrice exists\n",
    "    if 'totalPrice' in df.columns:\n",
    "        price_aggs = [\n",
    "            # Price position within searches (percentile rank)\n",
    "            (pl.col('totalPrice').rank(method='ordinal').over('ranker_id') /\n",
    "             pl.col('totalPrice').count().over('ranker_id')).mean().alias('price_position_preference'),\n",
    "        ]\n",
    "        customer_aggs.extend(price_aggs)\n",
    "\n",
    "    # Use lazy evaluation just like your original function\n",
    "    try:\n",
    "        # Create lazy frame and group by profileId (following your original pattern)\n",
    "        lazy_df = df.lazy().group_by('profileId')\n",
    "\n",
    "        # Apply feature aggregations\n",
    "        customer_features = lazy_df.agg(customer_aggs)\n",
    "\n",
    "        # Collect to materialize the results\n",
    "        enhanced_features = customer_features.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in lazy aggregation: {e}\")\n",
    "        # Fallback to simpler aggregation with lazy evaluation\n",
    "        simple_aggs = [\n",
    "            pl.len().alias('total_searches'),\n",
    "            pl.col('legs1_departureAt').is_not_null().mean().alias('roundtrip_preference'),\n",
    "            pl.col('searchRoute').drop_nulls().n_unique().alias('unique_routes_searched'),\n",
    "            pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().mode().first().alias('most_common_departure_airport'),\n",
    "        ]\n",
    "        lazy_df = df.lazy().group_by('profileId')\n",
    "        enhanced_features = lazy_df.agg(simple_aggs).collect()\n",
    "\n",
    "    print(f\"Generated {len(enhanced_features.columns)} customer features for {len(enhanced_features)} customers\")\n",
    "    return enhanced_features\n",
    "\n",
    "\n",
    "# Alternative: Use your original function directly for consistency\n",
    "def use_original_extract_customer_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Wrapper to use your original extract_customer_features function.\n",
    "    This ensures complete compatibility with your existing feature engineering.\n",
    "    \"\"\"\n",
    "    print(\"Using original extract_customer_features function...\")\n",
    "\n",
    "    try:\n",
    "        # Call your original function directly\n",
    "        customer_features = extract_customer_features(df)\n",
    "        return customer_features\n",
    "    except Exception as e:\n",
    "        print(f\"Original function failed: {e}\")\n",
    "        print(\"Falling back to scalable version...\")\n",
    "        return extract_customer_features_scalable(df)\n",
    "\n",
    "\n",
    "# Updated training function with choice of feature extraction\n",
    "def train_scalable_model_with_original_features(train_df: pl.DataFrame, test_df: pl.DataFrame = None,\n",
    "                                              use_original_features: bool = True) -> Tuple[np.ndarray, ScalableFlightRecommendationModel]:\n",
    "    \"\"\"\n",
    "    Training pipeline with option to use your original feature extraction.\n",
    "\n",
    "    Args:\n",
    "        use_original_features: If True, uses your original extract_customer_features function\n",
    "                             If False, uses the optimized scalable version\n",
    "    \"\"\"\n",
    "\n",
    "    if use_original_features:\n",
    "        print(\"Using your original extract_customer_features function...\")\n",
    "        train_customer_features = use_original_extract_customer_features(train_df)\n",
    "    else:\n",
    "        print(\"Using optimized scalable feature extraction...\")\n",
    "        train_customer_features = extract_customer_features_scalable(train_df)\n",
    "\n",
    "    print(\"Initializing scalable model...\")\n",
    "    model = ScalableFlightRecommendationModel(\n",
    "        n_customer_segments=50,\n",
    "        chunk_size=50000\n",
    "    )\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model.fit(train_df, train_customer_features)\n",
    "\n",
    "    predictions = None\n",
    "    if test_df is not None:\n",
    "        print(\"Making predictions...\")\n",
    "        if use_original_features:\n",
    "            test_customer_features = use_original_extract_customer_features(test_df)\n",
    "        else:\n",
    "            test_customer_features = extract_customer_features_scalable(test_df)\n",
    "        predictions = model.predict_proba_chunked(test_df, test_customer_features)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    model.save_model('scalable_flight_model.joblib')\n",
    "\n",
    "    return predictions, model\n",
    "\n",
    "\n",
    "# Usage example with performance monitoring\n",
    "print(PerformanceMonitor.estimate_memory_usage(18_000_000, 25))\n",
    "print(PerformanceMonitor.get_recommendations(18_000_000))\n",
    "\n",
    "# Recommend using the optimized version\n",
    "print(\"\\nFor 18M rows, use: train_scalable_model_optimized() instead of train_scalable_model()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
