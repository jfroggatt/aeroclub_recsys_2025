{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Customer Clustering",
   "id": "155d1264b8e89d0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "58773bdeb5f497ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T20:43:28.869615Z",
     "start_time": "2025-08-14T20:43:25.865206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ],
   "id": "987e4e5dd66336fa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data",
   "id": "a47d703235571d0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T20:45:04.326513Z",
     "start_time": "2025-08-14T20:45:01.524819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data = pl.concat([train, pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet')])\n",
    "data = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/train.parquet').drop('selected')"
   ],
   "id": "4b5b763b51581903",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T20:45:22.458767Z",
     "start_time": "2025-08-14T20:45:20.786147Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": "test_data = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet')",
   "id": "7d5018d919c8651a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T20:49:58.155648Z",
     "start_time": "2025-08-14T20:49:57.509915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_ids = test_data.join(data, on='profileId', how='anti')\n",
    "print(f\"The number of profiles in train data is {len(data.select('profileId').unique())}\")\n",
    "print(f\"The number of profiles in test data is {len(test_data.select('profileId').unique())}\")\n",
    "print(f\"the number of new profiles in test data is {len(new_ids.select(\"profileId\").unique())}\")"
   ],
   "id": "9e3c1e8873ed3d58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of profiles in train data is 32922\n",
      "The number of profiles in test data is 18981\n",
      "the number of new profiles in test data is 8281\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T20:47:28.428931Z",
     "start_time": "2025-08-14T20:47:28.419863Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6897776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6,
   "source": "len(test_data)",
   "id": "abcbbb12d48fb6a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utils",
   "id": "7816ea425c39cd74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:37:42.748066Z",
     "start_time": "2025-08-14T19:37:42.742037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def camel_to_snake(name):\n",
    "    \"\"\"Convert camelCase or PascalCase to snake_case\"\"\"\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    return s2.lower()\n",
    "\n",
    "def convert_columns_to_snake_case(df):\n",
    "    \"\"\"Convert all column names to snake_case\"\"\"\n",
    "    return df.rename({col: camel_to_snake(col) for col in df.columns})"
   ],
   "id": "b2ceffc32592c4b1",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:37:43.251626Z",
     "start_time": "2025-08-14T19:37:43.243485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_features_for_clustering(df, drop_cols=None):\n",
    "    # Features to drop\n",
    "    if drop_cols is None:\n",
    "        drop_cols = ['id', 'ranker_id', 'request_date']\n",
    "\n",
    "    # Identify categorical features (non-numeric columns)\n",
    "    categorical_features = [col for col, dtype in df.schema.items() if not dtype.is_numeric()]\n",
    "\n",
    "    # Using lazy computations to avoid memory issues\n",
    "    lazy_df = df.lazy()\n",
    "\n",
    "    # Encode categorical features\n",
    "    encoding_expressions = []\n",
    "    encoders = {}  # Mapping of feature name to encoding dictionary so we can evaluate cluster assignments\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        # Get unique values for the feature\n",
    "        unique_values = df[feature].fill_null('MISSING').unique().sort().to_list()\n",
    "\n",
    "        # Create mapping dictionary\n",
    "        encoders[feature] = {val: idx for idx, val in enumerate(unique_values)}\n",
    "\n",
    "        # Convert to categorical then to physical (integer codes)\n",
    "        encoding_expr = (\n",
    "            pl.col(feature)\n",
    "            .fill_null('MISSING')\n",
    "            .cast(pl.Categorical)\n",
    "            .to_physical()\n",
    "            .alias(f'{feature}_encoded')\n",
    "        )\n",
    "        encoding_expressions.append(encoding_expr)\n",
    "\n",
    "    # Apply encodings and exclude original categorical features\n",
    "    result_df = lazy_df.with_columns(encoding_expressions).select(pl.exclude(categorical_features + drop_cols)).fill_null(0).collect()\n",
    "\n",
    "    return result_df, encoders"
   ],
   "id": "ec1a3a804cacadde",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Engineer Customer Features",
   "id": "28b3983932e6b57"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T19:37:46.221229Z",
     "start_time": "2025-08-14T19:37:46.208259Z"
    }
   },
   "source": [
    "# Customer attributes for clustering analysis\n",
    "# CUSTOMER_ATTRIBUTES = ['profileId', 'companyID', 'sex', 'nationality', 'frequentFlyer', 'isVip', 'bySelf', 'corporateTariffCode']\n",
    "UNNEEDED_ATTRIBUTES = [\n",
    "    'ranker_id', 'isAccess3D', 'totalPrice', 'taxes', 'legs0_arrivalAt', 'legs0_duration', 'frequent_flyer',\n",
    "    r'^legs1_(departureAt|arrivalAt|duration)$'\n",
    "    r'^legs[01]_segments[0-3]_(operatingCarrier_code|aircraft_code|flightNumber)$',\n",
    "    r'^legs[01]_segments[0-3]_(arrivalTo|baggage|seats).*$'\n",
    "]\n",
    "POLARS_INDEX_COL = ['__index_level_0__']\n",
    "MAJOR_HUBS = ['ATL','DXB','DFW','HND','LHR','DEN','ORD','IST','PVG','ICN','CDG', 'JFK','CLT','MEX','SFO','EWR','MIA','BKK','GRU','HKG']\n",
    "\n",
    "\n",
    "def get_cabin_class_columns(df: pl.DataFrame) -> List[str]:\n",
    "    \"\"\"Get all cabin class columns from the dataframe.\"\"\"\n",
    "    columns = df.columns\n",
    "    return [col for col in columns if col.startswith('legs') and col.endswith('_cabinClass')]\n",
    "\n",
    "\n",
    "def create_customer_aggregation_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create customer aggregation expressions for basic attributes and search behavior.\"\"\"\n",
    "    return [\n",
    "        # Basic customer attributes (take first non-null value per customer)\n",
    "        pl.col('companyID').drop_nulls().first().alias('companyID'),\n",
    "        pl.col('sex').drop_nulls().first().alias('sex'),\n",
    "        pl.col('nationality').drop_nulls().first().alias('nationality'),\n",
    "        pl.col('frequentFlyer').drop_nulls().first().alias('frequentFlyer'),\n",
    "        pl.col('isVip').drop_nulls().first().alias('isVip'),\n",
    "        pl.col('bySelf').drop_nulls().first().alias('bySelf'),\n",
    "        pl.col('corporateTariffCode').drop_nulls().first().alias('corporateTariffCode'),\n",
    "\n",
    "        # Normalized frequentFlyer program, addressing null values as null strings, and translating UT program\n",
    "        pl.col('frequentFlyer').drop_nulls().first().str.replace('- ЮТэйр ЗАО', 'UT').fill_null('').alias('ff_normalized'),\n",
    "\n",
    "        # Search behavior metrics\n",
    "        pl.len().alias('total_searches'),\n",
    "        pl.col('legs1_departureAt').is_not_null().mean().alias('roundtrip_preference'),\n",
    "        pl.col('searchRoute').drop_nulls().n_unique().alias('unique_routes_searched'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_booking_lead_time_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create booking lead time statistics.\"\"\"\n",
    "    # Calculate booking lead time in days\n",
    "    booking_lead_expr = (\n",
    "        (pl.col('legs0_departureAt').str.to_datetime() -\n",
    "         pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1)\n",
    "    ).cast(pl.Int32)\n",
    "\n",
    "    return [\n",
    "        booking_lead_expr.min().alias('min_booking_lead_days'),\n",
    "        booking_lead_expr.max().alias('max_booking_lead_days'),\n",
    "        booking_lead_expr.mean().alias('avg_booking_lead_days'),\n",
    "        booking_lead_expr.median().alias('median_booking_lead_days'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_travel_preference_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create travel preference features for most common airports and carriers.\"\"\"\n",
    "    return [\n",
    "        # Most common departure airport\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().mode().first().alias('most_common_departure_airport'),\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').drop_nulls().n_unique().alias('unique_departure_airports'),\n",
    "\n",
    "        # Most common marketing carrier\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').drop_nulls().mode().first().alias('most_common_carrier'),\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').drop_nulls().n_unique().alias('unique_carriers_used'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_cabin_class_features(cabin_class_cols: List[str]) -> List[pl.Expr]:\n",
    "    \"\"\"Create cabin class preference statistics.\"\"\"\n",
    "    if not cabin_class_cols:\n",
    "        # Return default values if no cabin class columns found\n",
    "        return [\n",
    "            pl.lit(None).alias('min_cabin_class'),\n",
    "            pl.lit(None).alias('max_cabin_class'),\n",
    "            pl.lit(None).alias('avg_cabin_class'),\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        # Cabin class statistics across all segments\n",
    "        pl.min_horizontal([pl.col(col) for col in cabin_class_cols]).min().alias('min_cabin_class'),\n",
    "        pl.max_horizontal([pl.col(col) for col in cabin_class_cols]).max().alias('max_cabin_class'),\n",
    "        pl.mean_horizontal([pl.col(col) for col in cabin_class_cols]).mean().alias('avg_cabin_class'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_temporal_preference_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create temporal preference features for departure patterns.\"\"\"\n",
    "    return [\n",
    "        # Weekday preference (most common day of week for departures)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.weekday()\n",
    "          .mode().first().alias('weekday_preference'),\n",
    "\n",
    "        # Weekend travel rate (percentage of weekend departures - 5=Sat, 6=Sun)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.weekday()\n",
    "          .map_elements(lambda x: 1 if x >= 5 else 0, return_dtype=pl.Int8)\n",
    "          .mean().alias('weekend_travel_rate'),\n",
    "\n",
    "        # Time of day variance (how consistent are their departure times)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.hour()\n",
    "          .std().alias('time_of_day_variance'),\n",
    "\n",
    "        # Night flight preference (flights departing 22:00-06:00)\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.hour()\n",
    "          .map_elements(lambda x: 1 if (x >= 22 or x < 6) else 0, return_dtype=pl.Int8)\n",
    "          .mean().alias('night_flight_preference')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_route_specific_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create features related to route preferences and characteristics.\"\"\"\n",
    "\n",
    "    return [\n",
    "        # Route loyalty (how frequently they search the same routes)\n",
    "        (pl.col('searchRoute').n_unique() / pl.len())\n",
    "          .map_elements(lambda x: 1 - x if x > 0 else 0)  # Invert so higher = more loyal\n",
    "          .alias('route_loyalty'),\n",
    "\n",
    "        # Hub preference (preference for major hub airports)\n",
    "        pl.concat_list([\n",
    "            pl.col('legs0_segments0_departureFrom_airport_iata').is_in(MAJOR_HUBS),\n",
    "            pl.col('legs0_segments0_arrivalTo_airport_iata').is_in(MAJOR_HUBS)\n",
    "        ]).list.mean().alias('hub_preference'),\n",
    "\n",
    "        # Connection tolerance (preference for flights with connections)\n",
    "        pl.col('total_segments').mean().alias('connection_tolerance'),\n",
    "\n",
    "        # Short haul preference\n",
    "        (1 - (pl.col('legs0_duration').str.extract(r'^(\\d+):(\\d+)', 1).cast(pl.Int32) / 12))\n",
    "          .clip(0, 1).alias('short_haul_preference'),\n",
    "\n",
    "        # Domestic/international ratio based on route length\n",
    "        # Assuming routes with same first letter in IATA codes are likely domestic\n",
    "        pl.col('searchRoute').map_elements(\n",
    "            lambda route: 1 if route and route[:1] == route[3:4] else 0,\n",
    "            return_dtype=pl.Int8\n",
    "        ).mean().alias('domestic_international_ratio')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_price_sensitivity_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create features related to price sensitivity and patterns.\"\"\"\n",
    "    return [\n",
    "        # Price position preference (typical percentile chosen)\n",
    "        pl.col('price_percentile').mean().alias('price_position_preference'),\n",
    "\n",
    "        # Price to duration sensitivity\n",
    "        # Higher values mean more willing to pay for shorter flights\n",
    "        pl.covar(\n",
    "            pl.col('totalPrice'),\n",
    "            pl.col('total_duration') * -1  # Negative so higher = more sensitive\n",
    "        ).alias('price_to_duration_sensitivity'),\n",
    "\n",
    "        # Premium economy preference (assuming cabin class 2 is premium economy)\n",
    "        pl.mean_horizontal([\n",
    "            pl.col(f'legs0_segments{i}_cabinClass') == 2\n",
    "            for i in range(4)\n",
    "        ]).mean().alias('premium_economy_preference'),\n",
    "\n",
    "        # Consistent price tier (lower variance = more consistent)\n",
    "        pl.col('price_tier').std().map_elements(\n",
    "            lambda x: 1 - min(x / 3, 1) if x is not None else 0.5  # Invert and normalize\n",
    "        ).alias('consistent_price_tier')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_service_preference_features() -> List[pl.Expr]:\n",
    "    \"\"\"Create features related to service preferences.\"\"\"\n",
    "    return [\n",
    "        # Baggage preference (average selected baggage allowance)\n",
    "        pl.concat_list([\n",
    "            pl.col(f'legs0_segments{i}_baggageAllowance_quantity')\n",
    "            for i in range(4)\n",
    "        ]).list.mean().alias('baggage_preference'),\n",
    "\n",
    "        # Loyalty program utilization\n",
    "        pl.col('frequentFlyer').is_not_null().mean().alias('loyalty_program_utilization')\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_derived_metrics() -> List[pl.Expr]:\n",
    "    \"\"\"Create complex derived metrics from combinations of features.\"\"\"\n",
    "    return [\n",
    "        # Price flexibility index (higher price variance / booking rate = more flexible)\n",
    "        (pl.col('totalPrice').std() /\n",
    "         pl.col('booking_rate').clip(0.01, 1))\n",
    "        .alias('price_flexibility_index'),\n",
    "\n",
    "        # Convenience priority score (higher = more emphasis on convenient times)\n",
    "        ((1 - pl.col('time_of_day_variance')) * 10 +\n",
    "         pl.col('price_to_duration_sensitivity') * 5)\n",
    "        .alias('convenience_priority_score'),\n",
    "\n",
    "        # Loyalty vs price index (higher = more loyal, less price sensitive)\n",
    "        (pl.col('loyalty_program_utilization') * 10 -\n",
    "         pl.col('price_position_preference') / 10)\n",
    "        .alias('loyalty_vs_price_index'),\n",
    "\n",
    "        # Planning consistency score (inverse of lead time variance)\n",
    "        (1 / (pl.col('max_booking_lead_days') - pl.col('min_booking_lead_days') + 1))\n",
    "        .alias('planning_consistency_score'),\n",
    "\n",
    "        # Luxury index (combination of cabin class and price tier)\n",
    "        (pl.col('avg_cabin_class') * 20 +\n",
    "         pl.col('price_position_preference') / 2)\n",
    "        .alias('luxury_index')\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_customer_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract customer features for clustering analysis.\n",
    "    Aggregates by profileId to create customer-level features.\n",
    "    \"\"\"\n",
    "    # Check if already processed\n",
    "    if df.height > 0 and 'total_searches' in df.columns:\n",
    "        return df\n",
    "\n",
    "    # Get cabin class columns\n",
    "    cabin_class_cols = [col for col in df.columns if col.startswith('legs') and col.endswith('_cabinClass')]\n",
    "\n",
    "    # Create lazy frame and group by profileId\n",
    "    lazy_df = df.lazy().group_by('profileId')\n",
    "\n",
    "    # Apply feature groups\n",
    "    customer_features = lazy_df.agg([\n",
    "        *create_customer_aggregation_features(),\n",
    "        *create_booking_lead_time_features(),\n",
    "        *create_travel_preference_features(),\n",
    "        *create_cabin_class_features(cabin_class_cols),\n",
    "        *create_temporal_preference_features(),\n",
    "        *create_route_specific_features(),\n",
    "        *create_price_sensitivity_features(),\n",
    "        *create_service_preference_features()\n",
    "    ])\n",
    "\n",
    "    # Materialize to generate the basic features\n",
    "    base_features = customer_features.collect()\n",
    "\n",
    "    # Add the derived metrics that depend on the generated features\n",
    "    enhanced_features = base_features.with_columns(create_derived_metrics())\n",
    "\n",
    "    print(f\"Generated {len(enhanced_features.columns)} customer features for {len(enhanced_features)} customers\")\n",
    "    return enhanced_features\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_selection_data(df: pl.DataFrame, selections_df=None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the data to add selection information.\n",
    "    If selections_df is not provided, uses price as a proxy for selection.\n",
    "    \"\"\"\n",
    "    if selections_df is not None and 'selected' in selections_df.columns:\n",
    "        df_with_selections = df.with_columns(pl.Series('selected', selections_df['selected']))\n",
    "    else:\n",
    "        # Use price as a proxy - assume cheaper flights are more likely to be selected\n",
    "        df_with_selections = df.with_columns([\n",
    "            (pl.col('price_percentile') < 30).cast(pl.Int32).alias('selected')\n",
    "        ])\n",
    "\n",
    "    # Add search session ID\n",
    "    df_with_search = df_with_selections.with_columns([\n",
    "        pl.concat_str([\n",
    "            pl.col('profileId').cast(pl.Str),\n",
    "            pl.lit('_'),\n",
    "            pl.col('ranker_id'),\n",
    "            pl.lit('_'),\n",
    "            pl.col('requestDate')\n",
    "        ]).alias('search_session_id')\n",
    "    ])\n",
    "\n",
    "    return df_with_search\n"
   ],
   "id": "7aad3a2df3803bff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_customer_features(flight_df: pl.DataFrame, selections_df=None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate comprehensive customer features for flight recommendation.\n",
    "\n",
    "    Args:\n",
    "        flight_df: Raw flight search data\n",
    "        selections_df: Optional dataframe with 'selected' column indicating chosen flights\n",
    "\n",
    "    Returns:\n",
    "        Customer-level feature dataframe\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing flight data...\")\n",
    "    processed_df = preprocess_selection_data(flight_df, selections_df)\n",
    "\n",
    "    print(\"Extracting customer features...\")\n",
    "    customer_features = extract_customer_features(processed_df)\n",
    "\n",
    "    print(\"Finalizing features...\")\n",
    "    # Fill nulls and convert to appropriate types\n",
    "    final_features = customer_features.with_columns([\n",
    "        pl.all().fill_null(0)\n",
    "    ])\n",
    "\n",
    "    return final_features"
   ],
   "id": "9980d9d7473ce9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:37:55.389423Z",
     "start_time": "2025-08-14T19:37:48.151747Z"
    }
   },
   "cell_type": "code",
   "source": "cust_data = extract_customer_features(data)",
   "id": "d4419249cb348f3f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:37:59.861129Z",
     "start_time": "2025-08-14T19:37:59.851814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'cust_data shape: {cust_data.shape}')\n",
    "cust_data.head(100)"
   ],
   "id": "a1ff9c36c15f95da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_data shape: (32922, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shape: (100, 23)\n",
       "┌───────────┬───────────┬───────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ profileId ┆ companyID ┆ sex   ┆ nationalit ┆ … ┆ unique_car ┆ min_cabin_ ┆ max_cabin ┆ avg_cabin │\n",
       "│ ---       ┆ ---       ┆ ---   ┆ y          ┆   ┆ riers_used ┆ class      ┆ _class    ┆ _class    │\n",
       "│ i64       ┆ i64       ┆ bool  ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆           ┆       ┆ i64        ┆   ┆ u32        ┆ f64        ┆ f64       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 2877208   ┆ 61061     ┆ true  ┆ 36         ┆ … ┆ 1          ┆ 1.0        ┆ 2.0       ┆ 1.436364  │\n",
       "│ 2430627   ┆ 25515     ┆ false ┆ 36         ┆ … ┆ 1          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ 995557    ┆ 44071     ┆ true  ┆ 36         ┆ … ┆ 3          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ 3392490   ┆ 42620     ┆ false ┆ 36         ┆ … ┆ 1          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ 3580062   ┆ 56912     ┆ false ┆ 36         ┆ … ┆ 3          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ …         ┆ …         ┆ …     ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
       "│ 1422625   ┆ 36948     ┆ false ┆ 36         ┆ … ┆ 2          ┆ 1.0        ┆ 2.0       ┆ 1.057143  │\n",
       "│ 3421718   ┆ 60482     ┆ true  ┆ 47         ┆ … ┆ 2          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ 2924922   ┆ 42702     ┆ true  ┆ 36         ┆ … ┆ 5          ┆ 1.0        ┆ 2.0       ┆ 1.241935  │\n",
       "│ 2101869   ┆ 54163     ┆ false ┆ 36         ┆ … ┆ 8          ┆ 1.0        ┆ 2.0       ┆ 1.36653   │\n",
       "│ 343960    ┆ 24728     ┆ false ┆ 36         ┆ … ┆ 2          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "└───────────┴───────────┴───────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100, 23)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>profileId</th><th>companyID</th><th>sex</th><th>nationality</th><th>frequentFlyer</th><th>isVip</th><th>bySelf</th><th>corporateTariffCode</th><th>ff_normalized</th><th>total_searches</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>most_common_departure_airport</th><th>unique_departure_airports</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>min_cabin_class</th><th>max_cabin_class</th><th>avg_cabin_class</th></tr><tr><td>i64</td><td>i64</td><td>bool</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>i64</td><td>str</td><td>u32</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2877208</td><td>61061</td><td>true</td><td>36</td><td>&quot;SU&quot;</td><td>false</td><td>true</td><td>161</td><td>&quot;SU&quot;</td><td>110</td><td>1.0</td><td>2</td><td>6</td><td>17</td><td>7.054545</td><td>7.0</td><td>&quot;NBC&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>2.0</td><td>1.436364</td></tr><tr><td>2430627</td><td>25515</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>166</td><td>&quot;&quot;</td><td>33</td><td>1.0</td><td>1</td><td>14</td><td>14</td><td>14.0</td><td>14.0</td><td>&quot;ARH&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>995557</td><td>44071</td><td>true</td><td>36</td><td>&quot;SU/S7/N4&quot;</td><td>false</td><td>true</td><td>113</td><td>&quot;SU/S7/N4&quot;</td><td>60</td><td>1.0</td><td>1</td><td>28</td><td>28</td><td>28.0</td><td>28.0</td><td>&quot;KJA&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>3</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3392490</td><td>42620</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>63</td><td>1.0</td><td>1</td><td>10</td><td>11</td><td>10.746032</td><td>11.0</td><td>&quot;KUF&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3580062</td><td>56912</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>75</td><td>&quot;&quot;</td><td>117</td><td>1.0</td><td>1</td><td>1</td><td>2</td><td>1.726496</td><td>2.0</td><td>&quot;SVO&quot;</td><td>2</td><td>&quot;SU&quot;</td><td>3</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1422625</td><td>36948</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>153</td><td>&quot;&quot;</td><td>105</td><td>1.0</td><td>1</td><td>6</td><td>20</td><td>13.314286</td><td>12.0</td><td>&quot;SVO&quot;</td><td>1</td><td>&quot;DP&quot;</td><td>2</td><td>1.0</td><td>2.0</td><td>1.057143</td></tr><tr><td>3421718</td><td>60482</td><td>true</td><td>47</td><td>null</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>5</td><td>1.0</td><td>1</td><td>18</td><td>18</td><td>18.0</td><td>18.0</td><td>&quot;KJA&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>2</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>2924922</td><td>42702</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>1116</td><td>1.0</td><td>1</td><td>5</td><td>6</td><td>5.491935</td><td>5.0</td><td>&quot;VKO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>5</td><td>1.0</td><td>2.0</td><td>1.241935</td></tr><tr><td>2101869</td><td>54163</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>101</td><td>&quot;&quot;</td><td>487</td><td>0.889117</td><td>4</td><td>0</td><td>21</td><td>3.753593</td><td>3.0</td><td>&quot;SVO&quot;</td><td>7</td><td>&quot;SU&quot;</td><td>8</td><td>1.0</td><td>2.0</td><td>1.36653</td></tr><tr><td>343960</td><td>24728</td><td>false</td><td>36</td><td>&quot;SU/EY/S7&quot;</td><td>false</td><td>true</td><td>161</td><td>&quot;SU/EY/S7&quot;</td><td>19</td><td>1.0</td><td>1</td><td>19</td><td>19</td><td>19.0</td><td>19.0</td><td>&quot;SVO&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>2</td><td>1.0</td><td>1.0</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:37:49.907036Z",
     "start_time": "2025-08-14T17:37:49.900037Z"
    }
   },
   "cell_type": "code",
   "source": "cust_data.columns",
   "id": "fe34d2e95666edc8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profileId',\n",
       " 'companyID',\n",
       " 'sex',\n",
       " 'nationality',\n",
       " 'frequentFlyer',\n",
       " 'isVip',\n",
       " 'bySelf',\n",
       " 'corporateTarrifCode',\n",
       " 'ff_normalized',\n",
       " 'total_searches',\n",
       " 'roundtrip_preference',\n",
       " 'unique_routes_searched',\n",
       " 'min_booking_lead_days',\n",
       " 'max_booking_lead_days',\n",
       " 'avg_booking_lead_days',\n",
       " 'median_booking_lead_days',\n",
       " 'most_common_departure_airport',\n",
       " 'unique_departure_airports',\n",
       " 'most_common_carrier',\n",
       " 'unique_carriers_used',\n",
       " 'min_cabin_class',\n",
       " 'max_cabin_class',\n",
       " 'avg_cabin_class']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature Summary",
   "id": "2bdf7d7bc3f6a322"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def display_customer_feature_summary(customer_df: pl.DataFrame) -> None:\n",
    "    \"\"\"Display summary statistics for customer features.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CUSTOMER FEATURE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"Total unique customers: {customer_df.height:,}\")\n",
    "    print(f\"Total features: {len(customer_df.columns)}\")\n",
    "\n",
    "    # Categorical features summary\n",
    "    categorical_features = ['sex', 'nationality', 'frequentFlyer', 'corporateTarrifCode',\n",
    "                           'most_common_departure_airport', 'most_common_carrier']\n",
    "\n",
    "    print(\"\\nCATEGORICAL FEATURES:\")\n",
    "    for feature in categorical_features:\n",
    "        if feature in customer_df.columns:\n",
    "            value_counts = customer_df[feature].value_counts().head(5)\n",
    "            print(f\"\\n{feature}:\")\n",
    "            for row in value_counts.iter_rows():\n",
    "                print(f\"  {row[0]}: {row[1]:,}\")\n",
    "\n",
    "    # Numerical features summary\n",
    "    numerical_features = ['total_searches', 'avg_booking_lead_days', 'unique_routes_searched',\n",
    "                         'unique_departure_airports', 'unique_carriers_used',\n",
    "                         'min_cabin_class', 'max_cabin_class', 'avg_cabin_class']\n",
    "\n",
    "    print(f\"\\nNUMERICAL FEATURES:\")\n",
    "    for feature in numerical_features:\n",
    "        if feature in customer_df.columns:\n",
    "            stats = customer_df[feature].describe()\n",
    "            print(f\"\\n{feature}:\")\n",
    "            for row in stats.iter_rows():\n",
    "                print(f\"  {row[0]}: {row[1]:.2f}\" if row[1] is not None else f\"  {row[0]}: None\")\n",
    "\n",
    "    # Boolean features summary\n",
    "    boolean_features = ['isVip', 'bySelf']\n",
    "    print(f\"\\nBOOLEAN FEATURES:\")\n",
    "    for feature in boolean_features:\n",
    "        if feature in customer_df.columns:\n",
    "            true_pct = customer_df[feature].mean() * 100 if customer_df[feature].mean() is not None else 0\n",
    "            print(f\"{feature}: {true_pct:.1f}% True\")\n",
    "\n",
    "    print(f\"\\nRoundtrip preference: {customer_df['roundtrip_preference'].mean() * 100:.1f}% of searches are roundtrip\")"
   ],
   "id": "9efaf7e986c2312d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display_customer_feature_summary(cust_data)",
   "id": "e03b64dce9b3f4a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:38:16.902215Z",
     "start_time": "2025-08-14T19:38:16.885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cust_data = convert_columns_to_snake_case(cust_data)\n",
    "cust_data.head(20)"
   ],
   "id": "92985f70d7c8ab96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (20, 23)\n",
       "┌────────────┬────────────┬───────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ profile_id ┆ company_id ┆ sex   ┆ nationalit ┆ … ┆ unique_ca ┆ min_cabin ┆ max_cabin ┆ avg_cabin │\n",
       "│ ---        ┆ ---        ┆ ---   ┆ y          ┆   ┆ rriers_us ┆ _class    ┆ _class    ┆ _class    │\n",
       "│ i64        ┆ i64        ┆ bool  ┆ ---        ┆   ┆ ed        ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆       ┆ i64        ┆   ┆ ---       ┆ f64       ┆ f64       ┆ f64       │\n",
       "│            ┆            ┆       ┆            ┆   ┆ u32       ┆           ┆           ┆           │\n",
       "╞════════════╪════════════╪═══════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2877208    ┆ 61061      ┆ true  ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 2.0       ┆ 1.436364  │\n",
       "│ 2430627    ┆ 25515      ┆ false ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 995557     ┆ 44071      ┆ true  ┆ 36         ┆ … ┆ 3         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3392490    ┆ 42620      ┆ false ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3580062    ┆ 56912      ┆ false ┆ 36         ┆ … ┆ 3         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ …          ┆ …          ┆ …     ┆ …          ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 2089156    ┆ 27937      ┆ true  ┆ 36         ┆ … ┆ 14        ┆ 1.0       ┆ 2.0       ┆ 1.222091  │\n",
       "│ 1248945    ┆ 43507      ┆ true  ┆ 36         ┆ … ┆ 9         ┆ 1.0       ┆ 2.0       ┆ 1.117465  │\n",
       "│ 3226670    ┆ 44729      ┆ true  ┆ 36         ┆ … ┆ 8         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3570892    ┆ 53359      ┆ true  ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 2770639    ┆ 53375      ┆ true  ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "└────────────┴────────────┴───────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 23)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>profile_id</th><th>company_id</th><th>sex</th><th>nationality</th><th>frequent_flyer</th><th>is_vip</th><th>by_self</th><th>corporate_tariff_code</th><th>ff_normalized</th><th>total_searches</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>most_common_departure_airport</th><th>unique_departure_airports</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>min_cabin_class</th><th>max_cabin_class</th><th>avg_cabin_class</th></tr><tr><td>i64</td><td>i64</td><td>bool</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>i64</td><td>str</td><td>u32</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2877208</td><td>61061</td><td>true</td><td>36</td><td>&quot;SU&quot;</td><td>false</td><td>true</td><td>161</td><td>&quot;SU&quot;</td><td>110</td><td>1.0</td><td>2</td><td>6</td><td>17</td><td>7.054545</td><td>7.0</td><td>&quot;NBC&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>2.0</td><td>1.436364</td></tr><tr><td>2430627</td><td>25515</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>166</td><td>&quot;&quot;</td><td>33</td><td>1.0</td><td>1</td><td>14</td><td>14</td><td>14.0</td><td>14.0</td><td>&quot;ARH&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>995557</td><td>44071</td><td>true</td><td>36</td><td>&quot;SU/S7/N4&quot;</td><td>false</td><td>true</td><td>113</td><td>&quot;SU/S7/N4&quot;</td><td>60</td><td>1.0</td><td>1</td><td>28</td><td>28</td><td>28.0</td><td>28.0</td><td>&quot;KJA&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>3</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3392490</td><td>42620</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>63</td><td>1.0</td><td>1</td><td>10</td><td>11</td><td>10.746032</td><td>11.0</td><td>&quot;KUF&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3580062</td><td>56912</td><td>false</td><td>36</td><td>null</td><td>false</td><td>true</td><td>75</td><td>&quot;&quot;</td><td>117</td><td>1.0</td><td>1</td><td>1</td><td>2</td><td>1.726496</td><td>2.0</td><td>&quot;SVO&quot;</td><td>2</td><td>&quot;SU&quot;</td><td>3</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2089156</td><td>27937</td><td>true</td><td>36</td><td>&quot;SU/S7/TK&quot;</td><td>false</td><td>true</td><td>24</td><td>&quot;SU/S7/TK&quot;</td><td>1699</td><td>0.676869</td><td>5</td><td>1</td><td>22</td><td>13.703943</td><td>11.0</td><td>&quot;LED&quot;</td><td>4</td><td>&quot;TK&quot;</td><td>14</td><td>1.0</td><td>2.0</td><td>1.222091</td></tr><tr><td>1248945</td><td>43507</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>112</td><td>&quot;&quot;</td><td>376</td><td>0.68617</td><td>3</td><td>7</td><td>16</td><td>13.162234</td><td>15.0</td><td>&quot;MRV&quot;</td><td>3</td><td>&quot;S7&quot;</td><td>9</td><td>1.0</td><td>2.0</td><td>1.117465</td></tr><tr><td>3226670</td><td>44729</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>161</td><td>&quot;&quot;</td><td>42</td><td>0.0</td><td>2</td><td>5</td><td>13</td><td>11.547619</td><td>12.5</td><td>&quot;TJM&quot;</td><td>4</td><td>&quot;SU&quot;</td><td>8</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3570892</td><td>53359</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>4</td><td>1.0</td><td>1</td><td>6</td><td>6</td><td>6.0</td><td>6.0</td><td>&quot;ODO&quot;</td><td>1</td><td>&quot;IO&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>2770639</td><td>53375</td><td>true</td><td>36</td><td>&quot;SU/UT&quot;</td><td>false</td><td>true</td><td>null</td><td>&quot;SU/UT&quot;</td><td>2</td><td>0.0</td><td>2</td><td>1</td><td>3</td><td>2.0</td><td>2.0</td><td>&quot;VEO&quot;</td><td>2</td><td>&quot;KV&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encode data",
   "id": "f1ed33367cb87096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode categorical features\n",
    "drop_cust_cols = ['profile_id']\n",
    "cust_clustering_df, cust_encoders = prepare_features_for_clustering(cust_data, drop_cust_cols)\n",
    "\n",
    "# Convert to Pandas DataFrame for StandardScaler\n",
    "cust_clustering_pd_df = cust_clustering_df.to_pandas().fillna(0)"
   ],
   "id": "236ae11d1874918b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scale Data",
   "id": "5d3b528797352a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scale the features for use with KMeans\n",
    "scaler = StandardScaler()\n",
    "cust_features_scaled = scaler.fit_transform(cust_clustering_pd_df)\n",
    "\n",
    "del cust_clustering_pd_df\n",
    "gc.collect()"
   ],
   "id": "3030e68399dc7bfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cust_clustering_df.head(100)",
   "id": "7b916ccd7e035fc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run Cluster Modeling",
   "id": "a2947bddbc52ab85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use MiniBatchKMeans to cluster the data (more efficient for very large datasets)\n",
    "kmeans = MiniBatchKMeans(n_clusters=30, batch_size=50000, random_state=42)\n",
    "cust_clusters = kmeans.fit_predict(cust_features_scaled)"
   ],
   "id": "e892f39211edbf68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cust_clusters[:100]",
   "id": "84126ceb50e7608e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyze Clustering Results",
   "id": "90a870d518bd701d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_cluster_characteristics(customer_df: pl.DataFrame, cluster_col: str = 'cluster') -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze characteristics of each cluster in customer features dataframe.\n",
    "\n",
    "    Args:\n",
    "        customer_df: DataFrame with customer features and cluster assignments\n",
    "        cluster_col: Name of the column containing cluster labels\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with detailed cluster analysis\n",
    "    \"\"\"\n",
    "    if cluster_col not in customer_df.columns:\n",
    "        raise ValueError(f\"Cluster column '{cluster_col}' not found in dataframe\")\n",
    "\n",
    "    clusters = sorted(customer_df[cluster_col].unique().to_list())\n",
    "    analysis = {}\n",
    "\n",
    "    print(f\"Analyzing {len(clusters)} clusters...\")\n",
    "\n",
    "    for cluster_id in clusters:\n",
    "        cluster_data = customer_df.filter(pl.col(cluster_col) == cluster_id)\n",
    "        cluster_size = len(cluster_data)\n",
    "        cluster_pct = (cluster_size / len(customer_df)) * 100\n",
    "\n",
    "        print(f\"Processing Cluster {cluster_id}: {cluster_size:,} customers ({cluster_pct:.1f}%)\")\n",
    "\n",
    "        analysis[f'cluster_{cluster_id}'] = {\n",
    "            'size': cluster_size,\n",
    "            'percentage': cluster_pct,\n",
    "            'demographics': _analyze_demographics(cluster_data),\n",
    "            'search_behavior': _analyze_search_behavior(cluster_data),\n",
    "            'booking_patterns': _analyze_booking_patterns(cluster_data),\n",
    "            'travel_preferences': _analyze_travel_preferences(cluster_data),\n",
    "            'service_preferences': _analyze_service_preferences(cluster_data)\n",
    "        }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "def _analyze_demographics(cluster_data: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze demographic characteristics of a cluster.\"\"\"\n",
    "    demographics = {}\n",
    "\n",
    "    # Gender distribution\n",
    "    if 'sex' in cluster_data.columns:\n",
    "        sex_dist = cluster_data['sex'].value_counts().to_dict()\n",
    "        demographics['gender_distribution'] = sex_dist\n",
    "        demographics['dominant_gender'] = max(sex_dist, key=sex_dist.get) if sex_dist else None\n",
    "\n",
    "    # Nationality patterns\n",
    "    if 'nationality' in cluster_data.columns:\n",
    "        nationality_counts = cluster_data['nationality'].value_counts().head(5)\n",
    "        demographics['top_nationalities'] = dict(zip(\n",
    "            nationality_counts['nationality'].to_list(),\n",
    "            nationality_counts['count'].to_list()\n",
    "        ))\n",
    "        total_customers = len(cluster_data)\n",
    "        top_nationality_pct = (nationality_counts['count'][0] / total_customers * 100) if len(nationality_counts) > 0 else 0\n",
    "        demographics['nationality_concentration'] = top_nationality_pct\n",
    "\n",
    "    # VIP status\n",
    "    if 'is_vip' in cluster_data.columns:\n",
    "        vip_pct = cluster_data['is_vip'].mean() * 100 if cluster_data['is_vip'].mean() is not None else 0\n",
    "        demographics['vip_percentage'] = vip_pct\n",
    "        demographics['customer_tier'] = 'Premium' if vip_pct > 50 else 'Standard'\n",
    "\n",
    "    # Travel companion preference\n",
    "    if 'by_self' in cluster_data.columns:\n",
    "        solo_pct = cluster_data['by_self'].mean() * 100 if cluster_data['by_self'].mean() is not None else 0\n",
    "        demographics['solo_traveler_percentage'] = solo_pct\n",
    "        demographics['travel_style'] = 'Solo-oriented' if solo_pct > 60 else 'Group-oriented' if solo_pct < 40 else 'Mixed'\n",
    "\n",
    "    # Corporate vs leisure\n",
    "    if 'corporate_tariff_code' in cluster_data.columns:\n",
    "        corporate_pct = (cluster_data['corporate_tariff_code'].is_not_null().sum() / len(cluster_data)) * 100\n",
    "        demographics['corporate_percentage'] = corporate_pct\n",
    "        demographics['customer_type'] = 'Corporate' if corporate_pct > 70 else 'Leisure' if corporate_pct < 30 else 'Mixed'\n",
    "\n",
    "    return demographics\n",
    "\n",
    "\n",
    "def _analyze_search_behavior(cluster_data: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze search behavior patterns of a cluster.\"\"\"\n",
    "    behavior = {}\n",
    "\n",
    "    # Search frequency\n",
    "    if 'total_searches' in cluster_data.columns:\n",
    "        search_stats = cluster_data['total_searches'].describe()\n",
    "        behavior['avg_searches'] = search_stats.filter(pl.col('statistic') == 'mean')['value'][0]\n",
    "        behavior['median_searches'] = search_stats.filter(pl.col('statistic') == '50%')['value'][0]\n",
    "        behavior['max_searches'] = search_stats.filter(pl.col('statistic') == 'max')['value'][0]\n",
    "\n",
    "        # Categorize search intensity\n",
    "        avg_searches = behavior['avg_searches']\n",
    "        if avg_searches < 2:\n",
    "            behavior['search_intensity'] = 'Infrequent'\n",
    "        elif avg_searches < 5:\n",
    "            behavior['search_intensity'] = 'Moderate'\n",
    "        elif avg_searches < 10:\n",
    "            behavior['search_intensity'] = 'Active'\n",
    "        else:\n",
    "            behavior['search_intensity'] = 'Power User'\n",
    "\n",
    "    # Route diversity\n",
    "    if 'unique_routes_searched' in cluster_data.columns:\n",
    "        route_stats = cluster_data['unique_routes_searched'].describe()\n",
    "        behavior['avg_unique_routes'] = route_stats.filter(pl.col('statistic') == 'mean')['value'][0]\n",
    "        behavior['route_exploration'] = 'Explorer' if behavior['avg_unique_routes'] > 3 else 'Routine' if behavior['avg_unique_routes'] < 1.5 else 'Moderate'\n",
    "\n",
    "    # Trip type preference\n",
    "    if 'roundtrip_preference' in cluster_data.columns:\n",
    "        roundtrip_pct = cluster_data['roundtrip_preference'].mean() * 100\n",
    "        behavior['roundtrip_percentage'] = roundtrip_pct\n",
    "        behavior['trip_type_preference'] = 'Roundtrip-focused' if roundtrip_pct > 70 else 'One-way-focused' if roundtrip_pct < 30 else 'Mixed'\n",
    "\n",
    "    return behavior\n",
    "\n",
    "\n",
    "def _analyze_booking_patterns(cluster_data: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze booking lead time patterns of a cluster.\"\"\"\n",
    "    patterns = {}\n",
    "\n",
    "    lead_time_cols = ['min_booking_lead_days', 'max_booking_lead_days', 'avg_booking_lead_days', 'median_booking_lead_days']\n",
    "\n",
    "    for col in lead_time_cols:\n",
    "        if col in cluster_data.columns:\n",
    "            col_stats = cluster_data[col].describe()\n",
    "            patterns[col] = col_stats.filter(pl.col('statistic') == 'mean')['value'][0]\n",
    "\n",
    "    # Categorize booking behavior\n",
    "    if 'avg_booking_lead_days' in patterns:\n",
    "        avg_lead = patterns['avg_booking_lead_days']\n",
    "        if avg_lead < 7:\n",
    "            patterns['booking_style'] = 'Last-minute'\n",
    "        elif avg_lead < 30:\n",
    "            patterns['booking_style'] = 'Short-term planner'\n",
    "        elif avg_lead < 90:\n",
    "            patterns['booking_style'] = 'Advance planner'\n",
    "        else:\n",
    "            patterns['booking_style'] = 'Long-term planner'\n",
    "\n",
    "    # Booking consistency\n",
    "    if 'min_booking_lead_days' in patterns and 'max_booking_lead_days' in patterns:\n",
    "        lead_range = patterns['max_booking_lead_days'] - patterns['min_booking_lead_days']\n",
    "        patterns['booking_consistency'] = 'Consistent' if lead_range < 30 else 'Variable' if lead_range < 90 else 'Highly Variable'\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def _analyze_travel_preferences(cluster_data: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze travel preferences including airports and carriers.\"\"\"\n",
    "    preferences = {}\n",
    "\n",
    "    # Airport preferences\n",
    "    if 'most_common_departure_airport' in cluster_data.columns:\n",
    "        airport_counts = cluster_data['most_common_departure_airport'].value_counts().head(10)\n",
    "        preferences['top_departure_airports'] = dict(zip(\n",
    "            airport_counts['most_common_departure_airport'].to_list(),\n",
    "            airport_counts['count'].to_list()\n",
    "        ))\n",
    "\n",
    "    # Airport loyalty\n",
    "    if 'unique_departure_airports' in cluster_data.columns:\n",
    "        avg_airports = cluster_data['unique_departure_airports'].mean()\n",
    "        preferences['avg_airports_used'] = avg_airports\n",
    "        preferences['airport_loyalty'] = 'High' if avg_airports < 2 else 'Moderate' if avg_airports < 4 else 'Low'\n",
    "\n",
    "    # Carrier preferences\n",
    "    if 'most_common_carrier' in cluster_data.columns:\n",
    "        carrier_counts = cluster_data['most_common_carrier'].value_counts().head(10)\n",
    "        preferences['top_carriers'] = dict(zip(\n",
    "            carrier_counts['most_common_carrier'].to_list(),\n",
    "            carrier_counts['count'].to_list()\n",
    "        ))\n",
    "\n",
    "    # Carrier loyalty\n",
    "    if 'unique_carriers_used' in cluster_data.columns:\n",
    "        avg_carriers = cluster_data['unique_carriers_used'].mean()\n",
    "        preferences['avg_carriers_used'] = avg_carriers\n",
    "        preferences['carrier_loyalty'] = 'High' if avg_carriers < 2 else 'Moderate' if avg_carriers < 4 else 'Low'\n",
    "\n",
    "    # Frequent flyer programs\n",
    "    if 'ff_normalized' in cluster_data.columns:\n",
    "        ff_counts = cluster_data['ff_normalized'].value_counts().head(5)\n",
    "        preferences['frequent_flyer_programs'] = dict(zip(\n",
    "            ff_counts['ff_normalized'].to_list(),\n",
    "            ff_counts['count'].to_list()\n",
    "        ))\n",
    "        ff_participation = (cluster_data['ff_normalized'].is_not_null().sum() / len(cluster_data)) * 100\n",
    "        preferences['ff_participation_rate'] = ff_participation\n",
    "\n",
    "    return preferences\n",
    "\n",
    "\n",
    "def _analyze_service_preferences(cluster_data: pl.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze service level preferences (cabin class).\"\"\"\n",
    "    service = {}\n",
    "\n",
    "    cabin_cols = ['min_cabin_class', 'max_cabin_class', 'avg_cabin_class']\n",
    "\n",
    "    for col in cabin_cols:\n",
    "        if col in cluster_data.columns:\n",
    "            col_stats = cluster_data[col].describe()\n",
    "            service[col] = col_stats.filter(pl.col('statistic') == 'mean')['value'][0]\n",
    "\n",
    "    # Service level categorization\n",
    "    if 'avg_cabin_class' in service:\n",
    "        avg_cabin = service['avg_cabin_class']\n",
    "        if avg_cabin < 1.5:\n",
    "            service['service_preference'] = 'Economy-focused'\n",
    "        elif avg_cabin < 2.5:\n",
    "            service['service_preference'] = 'Premium Economy preferred'\n",
    "        elif avg_cabin < 3.5:\n",
    "            service['service_preference'] = 'Business class oriented'\n",
    "        else:\n",
    "            service['service_preference'] = 'First class oriented'\n",
    "\n",
    "    # Service consistency\n",
    "    if 'min_cabin_class' in service and 'max_cabin_class' in service:\n",
    "        cabin_range = service['max_cabin_class'] - service['min_cabin_class']\n",
    "        service['service_consistency'] = 'Consistent' if cabin_range < 1 else 'Flexible' if cabin_range < 2 else 'Highly Variable'\n",
    "\n",
    "    return service\n",
    "\n",
    "\n",
    "def display_cluster_interpretation(cluster_analysis: Dict) -> None:\n",
    "    \"\"\"Display comprehensive cluster interpretation.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CUSTOMER CLUSTER INTERPRETATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    total_clusters = len(cluster_analysis)\n",
    "\n",
    "    for cluster_name, analysis in cluster_analysis.items():\n",
    "        cluster_id = cluster_name.split('_')[1]\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLUSTER {cluster_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Size: {analysis['size']:,} customers ({analysis['percentage']:.1f}% of total)\")\n",
    "\n",
    "        # Demographics\n",
    "        demo = analysis['demographics']\n",
    "        print(f\"\\n📊 DEMOGRAPHICS:\")\n",
    "        if 'dominant_gender' in demo:\n",
    "            print(f\"  • Gender: {demo['dominant_gender']} dominant\")\n",
    "        if 'nationality_concentration' in demo:\n",
    "            print(f\"  • Nationality: {demo['nationality_concentration']:.1f}% concentration in top nationality\")\n",
    "        if 'customer_tier' in demo:\n",
    "            print(f\"  • Customer Tier: {demo['customer_tier']} ({demo.get('vip_percentage', 0):.1f}% VIP)\")\n",
    "        if 'travel_style' in demo:\n",
    "            print(f\"  • Travel Style: {demo['travel_style']} ({demo.get('solo_traveler_percentage', 0):.1f}% solo)\")\n",
    "        if 'customer_type' in demo:\n",
    "            print(f\"  • Customer Type: {demo['customer_type']} ({demo.get('corporate_percentage', 0):.1f}% corporate)\")\n",
    "\n",
    "        # Search Behavior\n",
    "        behavior = analysis['search_behavior']\n",
    "        print(f\"\\n🔍 SEARCH BEHAVIOR:\")\n",
    "        if 'search_intensity' in behavior:\n",
    "            print(f\"  • Search Intensity: {behavior['search_intensity']} ({behavior.get('avg_searches', 0):.1f} avg searches)\")\n",
    "        if 'route_exploration' in behavior:\n",
    "            print(f\"  • Route Exploration: {behavior['route_exploration']} ({behavior.get('avg_unique_routes', 0):.1f} avg routes)\")\n",
    "        if 'trip_type_preference' in behavior:\n",
    "            print(f\"  • Trip Type: {behavior['trip_type_preference']} ({behavior.get('roundtrip_percentage', 0):.1f}% roundtrip)\")\n",
    "\n",
    "        # Booking Patterns\n",
    "        booking = analysis['booking_patterns']\n",
    "        print(f\"\\n📅 BOOKING PATTERNS:\")\n",
    "        if 'booking_style' in booking:\n",
    "            print(f\"  • Booking Style: {booking['booking_style']} ({booking.get('avg_booking_lead_days', 0):.0f} days avg lead time)\")\n",
    "        if 'booking_consistency' in booking:\n",
    "            print(f\"  • Booking Consistency: {booking['booking_consistency']}\")\n",
    "\n",
    "        # Travel Preferences\n",
    "        travel = analysis['travel_preferences']\n",
    "        print(f\"\\n✈️ TRAVEL PREFERENCES:\")\n",
    "        if 'airport_loyalty' in travel:\n",
    "            print(f\"  • Airport Loyalty: {travel['airport_loyalty']} ({travel.get('avg_airports_used', 0):.1f} airports used)\")\n",
    "        if 'carrier_loyalty' in travel:\n",
    "            print(f\"  • Carrier Loyalty: {travel['carrier_loyalty']} ({travel.get('avg_carriers_used', 0):.1f} carriers used)\")\n",
    "        if 'ff_participation_rate' in travel:\n",
    "            print(f\"  • Frequent Flyer Participation: {travel['ff_participation_rate']:.1f}%\")\n",
    "\n",
    "        # Top preferences\n",
    "        if 'top_departure_airports' in travel and travel['top_departure_airports']:\n",
    "            top_airport = list(travel['top_departure_airports'].items())[0]\n",
    "            print(f\"  • Top Departure Airport: {top_airport[0]} ({top_airport[1]} customers)\")\n",
    "        if 'top_carriers' in travel and travel['top_carriers']:\n",
    "            top_carrier = list(travel['top_carriers'].items())[0]\n",
    "            print(f\"  • Top Carrier: {top_carrier[0]} ({top_carrier[1]} customers)\")\n",
    "\n",
    "        # Service Preferences\n",
    "        service = analysis['service_preferences']\n",
    "        print(f\"\\n🛂 SERVICE PREFERENCES:\")\n",
    "        if 'service_preference' in service:\n",
    "            print(f\"  • Service Level: {service['service_preference']} ({service.get('avg_cabin_class', 0):.2f} avg cabin class)\")\n",
    "        if 'service_consistency' in service:\n",
    "            print(f\"  • Service Consistency: {service['service_consistency']}\")\n",
    "\n",
    "\n",
    "def generate_cluster_personas(cluster_analysis: Dict) -> Dict:\n",
    "    \"\"\"Generate business personas for each cluster based on analysis.\"\"\"\n",
    "    personas = {}\n",
    "\n",
    "    for cluster_name, analysis in cluster_analysis.items():\n",
    "        cluster_id = cluster_name.split('_')[1]\n",
    "\n",
    "        # Extract key characteristics\n",
    "        demo = analysis['demographics']\n",
    "        behavior = analysis['search_behavior']\n",
    "        booking = analysis['booking_patterns']\n",
    "        travel = analysis['travel_preferences']\n",
    "        service = analysis['service_preferences']\n",
    "\n",
    "        # Generate persona name and description\n",
    "        persona_elements = []\n",
    "\n",
    "        # Add customer tier\n",
    "        if demo.get('customer_tier') == 'Premium':\n",
    "            persona_elements.append('Premium')\n",
    "\n",
    "        # Add booking style\n",
    "        booking_style = booking.get('booking_style', '')\n",
    "        if 'Last-minute' in booking_style:\n",
    "            persona_elements.append('Spontaneous')\n",
    "        elif 'Long-term' in booking_style:\n",
    "            persona_elements.append('Strategic')\n",
    "        elif 'Advance' in booking_style:\n",
    "            persona_elements.append('Planned')\n",
    "\n",
    "        # Add search intensity\n",
    "        search_intensity = behavior.get('search_intensity', '')\n",
    "        if search_intensity == 'Power User':\n",
    "            persona_elements.append('Power')\n",
    "        elif search_intensity == 'Infrequent':\n",
    "            persona_elements.append('Occasional')\n",
    "\n",
    "        # Add travel type\n",
    "        if demo.get('customer_type') == 'Corporate':\n",
    "            persona_elements.append('Business')\n",
    "        elif demo.get('travel_style') == 'Solo-oriented':\n",
    "            persona_elements.append('Independent')\n",
    "\n",
    "        # Add service preference\n",
    "        service_pref = service.get('service_preference', '')\n",
    "        if 'First class' in service_pref or 'Business class' in service_pref:\n",
    "            persona_elements.append('Luxury')\n",
    "        elif 'Economy' in service_pref:\n",
    "            persona_elements.append('Value')\n",
    "\n",
    "        # Create persona name\n",
    "        persona_name = ' '.join(persona_elements[:3]) + ' Traveler'\n",
    "        if not persona_elements:\n",
    "            persona_name = f'Segment {cluster_id} Traveler'\n",
    "\n",
    "        # Generate description\n",
    "        description_parts = []\n",
    "\n",
    "        if behavior.get('search_intensity'):\n",
    "            description_parts.append(f\"{behavior['search_intensity']} searchers\")\n",
    "\n",
    "        if booking.get('booking_style'):\n",
    "            description_parts.append(f\"who prefer {booking['booking_style'].lower()} booking\")\n",
    "\n",
    "        if service.get('service_preference'):\n",
    "            description_parts.append(f\"with {service['service_preference'].lower()}\")\n",
    "\n",
    "        if travel.get('carrier_loyalty'):\n",
    "            description_parts.append(f\"and {travel['carrier_loyalty'].lower()} carrier loyalty\")\n",
    "\n",
    "        description = ', '.join(description_parts[:4])\n",
    "\n",
    "        personas[cluster_name] = {\n",
    "            'persona_name': persona_name,\n",
    "            'description': description,\n",
    "            'size_percentage': analysis['percentage'],\n",
    "            'key_characteristics': {\n",
    "                'search_pattern': behavior.get('search_intensity', 'Unknown'),\n",
    "                'booking_style': booking.get('booking_style', 'Unknown'),\n",
    "                'service_preference': service.get('service_preference', 'Unknown'),\n",
    "                'loyalty_level': travel.get('carrier_loyalty', 'Unknown'),\n",
    "                'customer_tier': demo.get('customer_tier', 'Unknown')\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return personas\n",
    "\n",
    "\n",
    "def display_cluster_personas(personas: Dict) -> None:\n",
    "    \"\"\"Display business personas for clusters.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CUSTOMER PERSONAS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for cluster_name, persona in personas.items():\n",
    "        cluster_id = cluster_name.split('_')[1]\n",
    "\n",
    "        print(f\"\\n🎯 CLUSTER {cluster_id}: {persona['persona_name']}\")\n",
    "        print(f\"   Size: {persona['size_percentage']:.1f}% of customer base\")\n",
    "        print(f\"   Profile: {persona['description']}\")\n",
    "\n",
    "        chars = persona['key_characteristics']\n",
    "        print(f\"   • Search: {chars['search_pattern']}\")\n",
    "        print(f\"   • Booking: {chars['booking_style']}\")\n",
    "        print(f\"   • Service: {chars['service_preference']}\")\n",
    "        print(f\"   • Loyalty: {chars['loyalty_level']}\")\n",
    "        print(f\"   • Tier: {chars['customer_tier']}\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# cluster_analysis = analyze_cluster_characteristics(customer_features_with_clusters_df)\n",
    "# display_cluster_interpretation(cluster_analysis)\n",
    "# personas = generate_cluster_personas(cluster_analysis)\n",
    "# display_cluster_personas(personas)"
   ],
   "id": "6f19b63696aeeded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Add cluster labels back to your original customer data\n",
    "def add_clusters_to_customer_data(original_df: pl.DataFrame, cluster_labels: np.ndarray) -> pl.DataFrame:\n",
    "    \"\"\"Add cluster labels to the original customer features dataframe.\"\"\"\n",
    "\n",
    "    if len(cluster_labels) != len(original_df):\n",
    "        raise ValueError(f\"Cluster labels length ({len(cluster_labels)}) doesn't match dataframe length ({len(original_df)})\")\n",
    "\n",
    "    # Add cluster column\n",
    "    clustered_df = original_df.with_columns([\n",
    "        pl.Series(name='cluster', values=cluster_labels.astype(int))\n",
    "    ])\n",
    "\n",
    "    print(f\"Added cluster labels to {len(clustered_df)} customers\")\n",
    "    print(f\"Cluster distribution:\")\n",
    "    cluster_counts = clustered_df['cluster'].value_counts().sort('cluster')\n",
    "    for row in cluster_counts.iter_rows():\n",
    "        print(f\"  Cluster {row[0]}: {row[1]:,} customers ({row[1]/len(clustered_df)*100:.1f}%)\")\n",
    "\n",
    "    return clustered_df\n",
    "\n",
    "\n",
    "# Step 2: Quick cluster validation metrics\n",
    "def evaluate_clustering_quality(scaled_features: np.ndarray, cluster_labels: np.ndarray) -> Dict:\n",
    "    \"\"\"Evaluate the quality of clustering using standard metrics.\"\"\"\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics['silhouette_score'] = silhouette_score(scaled_features, cluster_labels)\n",
    "    metrics['calinski_harabasz_score'] = calinski_harabasz_score(scaled_features, cluster_labels)\n",
    "    metrics['davies_bouldin_score'] = davies_bouldin_score(scaled_features, cluster_labels)\n",
    "\n",
    "    # Number of clusters\n",
    "    metrics['n_clusters'] = len(np.unique(cluster_labels))\n",
    "\n",
    "    # Cluster sizes\n",
    "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    metrics['cluster_sizes'] = dict(zip(unique, counts))\n",
    "    metrics['min_cluster_size'] = counts.min()\n",
    "    metrics['max_cluster_size'] = counts.max()\n",
    "    metrics['avg_cluster_size'] = counts.mean()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def display_clustering_metrics(metrics: Dict) -> None:\n",
    "    \"\"\"Display clustering quality metrics.\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CLUSTERING QUALITY METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of clusters: {metrics['n_clusters']}\")\n",
    "    print(f\"Silhouette Score: {metrics['silhouette_score']:.4f} (higher is better, range: -1 to 1)\")\n",
    "    print(f\"Calinski-Harabasz Score: {metrics['calinski_harabasz_score']:.2f} (higher is better)\")\n",
    "    print(f\"Davies-Bouldin Score: {metrics['davies_bouldin_score']:.4f} (lower is better)\")\n",
    "    print(f\"\\nCluster Size Statistics:\")\n",
    "    print(f\"  Min cluster size: {metrics['min_cluster_size']:,}\")\n",
    "    print(f\"  Max cluster size: {metrics['max_cluster_size']:,}\")\n",
    "    print(f\"  Avg cluster size: {metrics['avg_cluster_size']:.0f}\")\n",
    "\n",
    "    # Interpretation\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    sil_score = metrics['silhouette_score']\n",
    "    if sil_score > 0.7:\n",
    "        sil_quality = \"Excellent\"\n",
    "    elif sil_score > 0.5:\n",
    "        sil_quality = \"Good\"\n",
    "    elif sil_score > 0.25:\n",
    "        sil_quality = \"Reasonable\"\n",
    "    else:\n",
    "        sil_quality = \"Poor\"\n",
    "\n",
    "    print(f\"  Silhouette score indicates {sil_quality} cluster separation\")\n",
    "\n",
    "    if metrics['min_cluster_size'] < 100:\n",
    "        print(f\"  Warning: Some clusters are very small (min: {metrics['min_cluster_size']})\")\n",
    "\n",
    "\n",
    "# Step 3: Decode cluster characteristics for interpretation\n",
    "def decode_categorical_features(clustered_df: pl.DataFrame, encoders: Dict) -> pl.DataFrame:\n",
    "    \"\"\"Decode categorical features back to original values for interpretation.\"\"\"\n",
    "\n",
    "    # Create reverse mapping for each encoder\n",
    "    reverse_encoders = {}\n",
    "    for feature, encoder_dict in encoders.items():\n",
    "        reverse_encoders[f'{feature}_encoded'] = {v: k for k, v in encoder_dict.items()}\n",
    "\n",
    "    # Decode categorical features\n",
    "    decode_expressions = []\n",
    "    for encoded_col, reverse_mapping in reverse_encoders.items():\n",
    "        # Create mapping expression\n",
    "        mapping_expr = pl.col(encoded_col).map_elements(\n",
    "            lambda x: reverse_mapping.get(x, 'Unknown'),\n",
    "            return_dtype=pl.Utf8\n",
    "        ).alias(encoded_col.replace('_encoded', '_decoded'))\n",
    "\n",
    "        decode_expressions.append(mapping_expr)\n",
    "\n",
    "    # Add decoded columns\n",
    "    if decode_expressions:\n",
    "        result_df = clustered_df.with_columns(decode_expressions)\n",
    "    else:\n",
    "        result_df = clustered_df\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Step 4: Complete workflow function\n",
    "def run_cluster_evaluation_workflow(original_customer_df: pl.DataFrame,\n",
    "                                  scaled_features: np.ndarray,\n",
    "                                  cluster_labels: np.ndarray,\n",
    "                                  encoders: Dict) -> Tuple[pl.DataFrame, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Complete workflow to evaluate and interpret clustering results.\n",
    "\n",
    "    Args:\n",
    "        original_customer_df: Original customer features dataframe\n",
    "        scaled_features: Scaled features used for clustering\n",
    "        cluster_labels: Cluster assignments from your clustering algorithm\n",
    "        encoders: Encoding dictionary from prepare_features_for_clustering\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (clustered_dataframe, quality_metrics, cluster_analysis)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting cluster evaluation workflow...\")\n",
    "\n",
    "    # Step 1: Add clusters to original data\n",
    "    clustered_df = add_clusters_to_customer_data(original_customer_df, cluster_labels)\n",
    "\n",
    "    # Step 2: Evaluate clustering quality\n",
    "    print(\"\\nEvaluating clustering quality...\")\n",
    "    quality_metrics = evaluate_clustering_quality(scaled_features, cluster_labels)\n",
    "    display_clustering_metrics(quality_metrics)\n",
    "\n",
    "    # Step 3: Decode categorical features for interpretation\n",
    "    print(\"\\nDecoding categorical features...\")\n",
    "    decoded_df = decode_categorical_features(clustered_df, encoders)\n",
    "\n",
    "    # Step 4: Analyze cluster characteristics\n",
    "    print(\"\\nAnalyzing cluster characteristics...\")\n",
    "    cluster_analysis = analyze_cluster_characteristics(decoded_df, cluster_col='cluster')\n",
    "\n",
    "    # Step 5: Display interpretation\n",
    "    display_cluster_interpretation(cluster_analysis)\n",
    "\n",
    "    # Step 6: Generate personas\n",
    "    print(\"\\nGenerating customer personas...\")\n",
    "    personas = generate_cluster_personas(cluster_analysis)\n",
    "    display_cluster_personas(personas)\n",
    "\n",
    "    return decoded_df, quality_metrics, cluster_analysis\n"
   ],
   "id": "b64f49eeb0b2cd64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add Cluster Labels to Encoded DataFrame",
   "id": "458af5963da32613"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add cluster labels to your ENCODED clustering dataframe\n",
    "clustered_encoded_df = add_clusters_to_customer_data(cust_clustering_df, cust_clusters)"
   ],
   "id": "335143fe1da0fb01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Eval Cluster Quality",
   "id": "9c708bff4ad7ebae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate clustering quality\n",
    "quality_metrics = evaluate_clustering_quality(cust_features_scaled, cust_clusters)\n",
    "display_clustering_metrics(quality_metrics)\n",
    "\n",
    "# Decode categorical features for better interpretation\n",
    "decoded_customer_df = decode_categorical_features(clustered_encoded_df, cust_encoders)\n",
    "\n",
    "# Run the full cluster analysis\n",
    "cluster_analysis = analyze_cluster_characteristics(decoded_customer_df, cluster_col='cluster')\n",
    "display_cluster_interpretation(cluster_analysis)\n",
    "\n",
    "# Generate business personas\n",
    "personas = generate_cluster_personas(cluster_analysis)\n",
    "display_cluster_personas(personas)"
   ],
   "id": "8f5f8efb20c9d5b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cluster Optimization",
   "id": "d424b2a96672be8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "# import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MiniBatchKMeansOptimizer:\n",
    "    \"\"\"\n",
    "    Optimizer specifically for MiniBatchKMeans on large flight datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scaled_features, current_labels=None):\n",
    "        \"\"\"\n",
    "        Initialize with your scaled features and current clustering results\n",
    "\n",
    "        Parameters:\n",
    "        scaled_features: numpy array or DataFrame of scaled customer features\n",
    "        current_labels: existing cluster labels (optional)\n",
    "        \"\"\"\n",
    "        self.scaled_features = scaled_features\n",
    "        self.current_labels = current_labels\n",
    "        self.n_samples, self.n_features = scaled_features.shape\n",
    "\n",
    "        print(f\"Initialized with {self.n_samples:,} samples and {self.n_features} features\")\n",
    "\n",
    "    def analyze_current_clustering(self):\n",
    "        \"\"\"Analyze your current clustering results\"\"\"\n",
    "        if self.current_labels is None:\n",
    "            print(\"No current clustering labels provided\")\n",
    "            return None\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CURRENT CLUSTERING ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Basic statistics\n",
    "        unique_labels, counts = np.unique(self.current_labels, return_counts=True)\n",
    "        n_clusters = len(unique_labels)\n",
    "\n",
    "        # Calculate metrics (on a sample for efficiency)\n",
    "        sample_size = min(100000, self.n_samples)\n",
    "        sample_indices = np.random.choice(self.n_samples, sample_size, replace=False)\n",
    "        sample_features = self.scaled_features[sample_indices]\n",
    "        sample_labels = self.current_labels[sample_indices]\n",
    "\n",
    "        sil_score = silhouette_score(sample_features, sample_labels)\n",
    "        ch_score = calinski_harabasz_score(sample_features, sample_labels)\n",
    "        db_score = davies_bouldin_score(sample_features, sample_labels)\n",
    "\n",
    "        print(f\"Metrics (calculated on {sample_size:,} sample):\")\n",
    "        print(f\"  Silhouette Score: {sil_score:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz Score: {ch_score:.2f}\")\n",
    "        print(f\"  Davies-Bouldin Score: {db_score:.4f}\")\n",
    "\n",
    "        # Cluster balance analysis\n",
    "        cv = counts.std() / counts.mean()\n",
    "        print(f\"\\nCluster Balance:\")\n",
    "        print(f\"  Number of clusters: {n_clusters}\")\n",
    "        print(f\"  Min cluster size: {counts.min():,}\")\n",
    "        print(f\"  Max cluster size: {counts.max():,}\")\n",
    "        print(f\"  Avg cluster size: {counts.mean():.0f}\")\n",
    "        print(f\"  Cluster imbalance (CV): {cv:.3f}\")\n",
    "\n",
    "        # Identify problematic clusters\n",
    "        large_threshold = np.percentile(counts, 90)\n",
    "        small_threshold = np.percentile(counts, 10)\n",
    "\n",
    "        large_clusters = unique_labels[counts > large_threshold]\n",
    "        small_clusters = unique_labels[counts < small_threshold]\n",
    "\n",
    "        print(f\"\\nProblematic Clusters:\")\n",
    "        print(f\"  Large clusters (>{large_threshold:.0f}): {large_clusters}\")\n",
    "        print(f\"  Small clusters (<{small_threshold:.0f}): {small_clusters}\")\n",
    "\n",
    "        return {\n",
    "            'metrics': {'silhouette': sil_score, 'ch_score': ch_score, 'db_score': db_score},\n",
    "            'cluster_sizes': counts,\n",
    "            'large_clusters': large_clusters,\n",
    "            'small_clusters': small_clusters,\n",
    "            'cluster_balance': cv\n",
    "        }\n",
    "\n",
    "    def optimize_hyperparameters(self, n_clusters_range=None, batch_sizes=None, n_trials=5):\n",
    "        \"\"\"\n",
    "        Optimize MiniBatchKMeans hyperparameters\n",
    "\n",
    "        Parameters:\n",
    "        n_clusters_range: list of cluster numbers to try\n",
    "        batch_sizes: list of batch sizes to try\n",
    "        n_trials: number of random initializations per configuration\n",
    "        \"\"\"\n",
    "        if n_clusters_range is None:\n",
    "            n_clusters_range = [20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "        if batch_sizes is None:\n",
    "            batch_sizes = [10000, 25000, 50000, 100000]\n",
    "\n",
    "        print(f\"\\n🔍 HYPERPARAMETER OPTIMIZATION\")\n",
    "        print(f\"Testing {len(n_clusters_range)} cluster counts × {len(batch_sizes)} batch sizes\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        results = []\n",
    "        best_score = -1\n",
    "        best_config = None\n",
    "\n",
    "        # Use sample for evaluation to speed up\n",
    "        eval_sample_size = min(50000, self.n_samples)\n",
    "        eval_indices = np.random.choice(self.n_samples, eval_sample_size, replace=False)\n",
    "        eval_features = self.scaled_features[eval_indices]\n",
    "\n",
    "        for n_clusters in n_clusters_range:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(f\"Testing n_clusters={n_clusters}, batch_size={batch_size:,}\")\n",
    "\n",
    "                trial_scores = []\n",
    "\n",
    "                for trial in range(n_trials):\n",
    "                    # Fit MiniBatchKMeans\n",
    "                    mbk = MiniBatchKMeans(\n",
    "                        n_clusters=n_clusters,\n",
    "                        batch_size=min(batch_size, self.n_samples),\n",
    "                        random_state=42 + trial,\n",
    "                        n_init=1,\n",
    "                        max_iter=100\n",
    "                    )\n",
    "\n",
    "                    # Fit on full data, evaluate on sample\n",
    "                    mbk.fit(self.scaled_features)\n",
    "                    eval_labels = mbk.predict(eval_features)\n",
    "\n",
    "                    # Calculate silhouette score\n",
    "                    sil_score = silhouette_score(eval_features, eval_labels)\n",
    "                    trial_scores.append(sil_score)\n",
    "\n",
    "                avg_score = np.mean(trial_scores)\n",
    "                std_score = np.std(trial_scores)\n",
    "\n",
    "                results.append({\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'batch_size': batch_size,\n",
    "                    'avg_silhouette': avg_score,\n",
    "                    'std_silhouette': std_score,\n",
    "                    'stability': 1 - (std_score / abs(avg_score)) if avg_score != 0 else 0\n",
    "                })\n",
    "\n",
    "                print(f\"  Avg silhouette: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    best_config = {\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'batch_size': batch_size,\n",
    "                        'silhouette': avg_score\n",
    "                    }\n",
    "\n",
    "        print(f\"\\n🏆 BEST CONFIGURATION:\")\n",
    "        print(f\"  Clusters: {best_config['n_clusters']}\")\n",
    "        print(f\"  Batch size: {best_config['batch_size']:,}\")\n",
    "        print(f\"  Silhouette score: {best_config['silhouette']:.4f}\")\n",
    "\n",
    "        return results, best_config\n",
    "\n",
    "    def apply_optimized_clustering(self, n_clusters, batch_size=50000, n_init=20):\n",
    "        \"\"\"\n",
    "        Apply optimized MiniBatchKMeans clustering\n",
    "\n",
    "        Parameters:\n",
    "        n_clusters: optimal number of clusters\n",
    "        batch_size: optimal batch size\n",
    "        n_init: number of initializations for better stability\n",
    "        \"\"\"\n",
    "        print(f\"\\n🚀 APPLYING OPTIMIZED CLUSTERING\")\n",
    "        print(f\"Parameters: {n_clusters} clusters, batch_size={batch_size:,}, n_init={n_init}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Use multiple initializations for better results\n",
    "        best_mbk = None\n",
    "        best_inertia = float('inf')\n",
    "\n",
    "        for init in range(n_init):\n",
    "            mbk = MiniBatchKMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                batch_size=min(batch_size, self.n_samples),\n",
    "                random_state=42 + init,\n",
    "                n_init=1,\n",
    "                max_iter=300,\n",
    "                tol=1e-6\n",
    "            )\n",
    "\n",
    "            mbk.fit(self.scaled_features)\n",
    "\n",
    "            if mbk.inertia_ < best_inertia:\n",
    "                best_inertia = mbk.inertia_\n",
    "                best_mbk = mbk\n",
    "\n",
    "            if (init + 1) % 5 == 0:\n",
    "                print(f\"  Completed {init + 1}/{n_init} initializations\")\n",
    "\n",
    "        # Get final labels\n",
    "        optimized_labels = best_mbk.predict(self.scaled_features)\n",
    "\n",
    "        # Calculate final metrics on sample\n",
    "        sample_size = min(50000, self.n_samples)\n",
    "        sample_indices = np.random.choice(self.n_samples, sample_size, replace=False)\n",
    "        sample_features = self.scaled_features[sample_indices]\n",
    "        sample_labels = optimized_labels[sample_indices]\n",
    "\n",
    "        final_sil = silhouette_score(sample_features, sample_labels)\n",
    "        final_ch = calinski_harabasz_score(sample_features, sample_labels)\n",
    "        final_db = davies_bouldin_score(sample_features, sample_labels)\n",
    "\n",
    "        print(f\"\\n📊 FINAL METRICS:\")\n",
    "        print(f\"  Silhouette Score: {final_sil:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz: {final_ch:.2f}\")\n",
    "        print(f\"  Davies-Bouldin: {final_db:.4f}\")\n",
    "        print(f\"  Inertia: {best_inertia:.2f}\")\n",
    "\n",
    "        return optimized_labels, best_mbk\n",
    "\n",
    "    def post_process_clusters(self, labels, min_cluster_size=500):\n",
    "        \"\"\"\n",
    "        Post-process clusters to handle imbalances\n",
    "\n",
    "        Parameters:\n",
    "        labels: cluster labels\n",
    "        min_cluster_size: minimum allowed cluster size\n",
    "        \"\"\"\n",
    "        print(f\"\\n🔧 POST-PROCESSING CLUSTERS\")\n",
    "        print(f\"Minimum cluster size: {min_cluster_size}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        # Identify small clusters\n",
    "        small_clusters = unique_labels[counts < min_cluster_size]\n",
    "\n",
    "        if len(small_clusters) == 0:\n",
    "            print(\"No small clusters found. No post-processing needed.\")\n",
    "            return labels\n",
    "\n",
    "        print(f\"Found {len(small_clusters)} small clusters: {small_clusters}\")\n",
    "\n",
    "        # Strategy: Merge small clusters with nearest larger clusters\n",
    "        processed_labels = labels.copy()\n",
    "\n",
    "        # Calculate cluster centers\n",
    "        cluster_centers = {}\n",
    "        for cluster_id in unique_labels:\n",
    "            mask = labels == cluster_id\n",
    "            cluster_centers[cluster_id] = np.mean(self.scaled_features[mask], axis=0)\n",
    "\n",
    "        # For each small cluster, find the nearest large cluster\n",
    "        for small_cluster in small_clusters:\n",
    "            if counts[unique_labels == small_cluster][0] < min_cluster_size:\n",
    "                small_center = cluster_centers[small_cluster]\n",
    "\n",
    "                # Find nearest large cluster\n",
    "                min_distance = float('inf')\n",
    "                nearest_large_cluster = None\n",
    "\n",
    "                for large_cluster in unique_labels:\n",
    "                    if large_cluster != small_cluster and counts[unique_labels == large_cluster][0] >= min_cluster_size:\n",
    "                        large_center = cluster_centers[large_cluster]\n",
    "                        distance = np.linalg.norm(small_center - large_center)\n",
    "\n",
    "                        if distance < min_distance:\n",
    "                            min_distance = distance\n",
    "                            nearest_large_cluster = large_cluster\n",
    "\n",
    "                # Merge small cluster into nearest large cluster\n",
    "                if nearest_large_cluster is not None:\n",
    "                    processed_labels[labels == small_cluster] = nearest_large_cluster\n",
    "                    print(f\"  Merged cluster {small_cluster} -> {nearest_large_cluster}\")\n",
    "\n",
    "        # Report final cluster distribution\n",
    "        final_unique, final_counts = np.unique(processed_labels, return_counts=True)\n",
    "        print(f\"\\nFinal cluster distribution:\")\n",
    "        print(f\"  Number of clusters: {len(final_unique)}\")\n",
    "        print(f\"  Min cluster size: {final_counts.min():,}\")\n",
    "        print(f\"  Max cluster size: {final_counts.max():,}\")\n",
    "        print(f\"  Avg cluster size: {final_counts.mean():.0f}\")\n",
    "\n",
    "        return processed_labels\n",
    "\n",
    "    def compare_clusterings(self, old_labels, new_labels, sample_size=50000):\n",
    "        \"\"\"\n",
    "        Compare old vs new clustering results\n",
    "\n",
    "        Parameters:\n",
    "        old_labels: original cluster labels\n",
    "        new_labels: new cluster labels\n",
    "        sample_size: sample size for metric calculation\n",
    "        \"\"\"\n",
    "        print(f\"\\n📈 CLUSTERING COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Sample for comparison\n",
    "        sample_indices = np.random.choice(len(old_labels), min(sample_size, len(old_labels)), replace=False)\n",
    "        sample_features = self.scaled_features[sample_indices]\n",
    "\n",
    "        # Calculate metrics for both\n",
    "        old_sample_labels = old_labels[sample_indices]\n",
    "        new_sample_labels = new_labels[sample_indices]\n",
    "\n",
    "        old_sil = silhouette_score(sample_features, old_sample_labels)\n",
    "        new_sil = silhouette_score(sample_features, new_sample_labels)\n",
    "\n",
    "        old_ch = calinski_harabasz_score(sample_features, old_sample_labels)\n",
    "        new_ch = calinski_harabasz_score(sample_features, new_sample_labels)\n",
    "\n",
    "        old_db = davies_bouldin_score(sample_features, old_sample_labels)\n",
    "        new_db = davies_bouldin_score(sample_features, new_sample_labels)\n",
    "\n",
    "        # Cluster balance\n",
    "        old_unique, old_counts = np.unique(old_labels, return_counts=True)\n",
    "        new_unique, new_counts = np.unique(new_labels, return_counts=True)\n",
    "\n",
    "        old_cv = old_counts.std() / old_counts.mean()\n",
    "        new_cv = new_counts.std() / new_counts.mean()\n",
    "\n",
    "        print(\"Metric Comparison:\")\n",
    "        print(f\"{'Metric':<20} {'Original':<12} {'Optimized':<12} {'Improvement':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Silhouette':<20} {old_sil:<12.4f} {new_sil:<12.4f} {((new_sil-old_sil)/abs(old_sil)*100):+.1f}%\")\n",
    "        print(f\"{'Calinski-Harabasz':<20} {old_ch:<12.2f} {new_ch:<12.2f} {((new_ch-old_ch)/old_ch*100):+.1f}%\")\n",
    "        print(f\"{'Davies-Bouldin':<20} {old_db:<12.4f} {new_db:<12.4f} {((old_db-new_db)/old_db*100):+.1f}%\")\n",
    "        print(f\"{'Cluster Balance':<20} {old_cv:<12.4f} {new_cv:<12.4f} {((old_cv-new_cv)/old_cv*100):+.1f}%\")\n",
    "        print(f\"{'Num Clusters':<20} {len(old_unique):<12} {len(new_unique):<12} {len(new_unique)-len(old_unique):+}\")\n",
    "\n",
    "        return {\n",
    "            'old_metrics': {'silhouette': old_sil, 'ch': old_ch, 'db': old_db, 'balance': old_cv},\n",
    "            'new_metrics': {'silhouette': new_sil, 'ch': new_ch, 'db': new_db, 'balance': new_cv}\n",
    "        }\n",
    "\n",
    "def quick_fixes_for_current_clustering(scaled_features, current_labels):\n",
    "    \"\"\"\n",
    "    Quick fixes you can apply to your current clustering without re-running everything\n",
    "    \"\"\"\n",
    "    print(\"⚡ QUICK FIXES FOR CURRENT CLUSTERING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    optimizer = MiniBatchKMeansOptimizer(scaled_features, current_labels)\n",
    "    analysis = optimizer.analyze_current_clustering()\n",
    "\n",
    "    if analysis is None:\n",
    "        return\n",
    "\n",
    "    # Apply post-processing\n",
    "    processed_labels = optimizer.post_process_clusters(current_labels, min_cluster_size=500)\n",
    "\n",
    "    # Compare results\n",
    "    if not np.array_equal(current_labels, processed_labels):\n",
    "        comparison = optimizer.compare_clusterings(current_labels, processed_labels)\n",
    "        return processed_labels, comparison\n",
    "    else:\n",
    "        print(\"No changes needed in post-processing.\")\n",
    "        return current_labels, None"
   ],
   "id": "860f8d2363b79917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage with your existing setup:\n",
    "# Your existing code:\n",
    "# kmeans = MiniBatchKMeans(n_clusters=30, batch_size=50000, random_state=42)\n",
    "# cust_clusters = kmeans.fit_predict(cust_features_scaled)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = MiniBatchKMeansOptimizer(cust_features_scaled, cust_clusters)\n",
    "\n",
    "# Analyze current results\n",
    "current_analysis = optimizer.analyze_current_clustering()\n",
    "\n",
    "# Find better hyperparameters\n",
    "optimization_results, best_config = optimizer.optimize_hyperparameters()\n",
    "\n",
    "# Apply optimized clustering\n",
    "new_labels, new_model = optimizer.apply_optimized_clustering(\n",
    "    n_clusters=best_config['n_clusters'],\n",
    "    batch_size=best_config['batch_size']\n",
    ")\n",
    "\n",
    "# Post-process to handle cluster imbalances\n",
    "final_labels = optimizer.post_process_clusters(new_labels, min_cluster_size=500)\n",
    "\n",
    "# Compare old vs new\n",
    "comparison = optimizer.compare_clusterings(cust_clusters, final_labels)\n",
    "\n",
    "# Quick fix for current clustering (alternative approach)\n",
    "# quick_labels, quick_comparison = quick_fixes_for_current_clustering(cust_features_scaled, cust_clusters)"
   ],
   "id": "f1facc637f7628ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Additional Optimization Efforts",
   "id": "8cc349633f556d5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:38:46.956018Z",
     "start_time": "2025-08-14T17:38:46.948729Z"
    }
   },
   "cell_type": "code",
   "source": "cust_data.head()",
   "id": "a3aabbcaee2d6d87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (5, 23)\n",
       "┌───────────┬───────────┬───────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ profileId ┆ companyID ┆ sex   ┆ nationalit ┆ … ┆ unique_car ┆ min_cabin_ ┆ max_cabin ┆ avg_cabin │\n",
       "│ ---       ┆ ---       ┆ ---   ┆ y          ┆   ┆ riers_used ┆ class      ┆ _class    ┆ _class    │\n",
       "│ i64       ┆ i64       ┆ bool  ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆           ┆       ┆ i64        ┆   ┆ u32        ┆ f64        ┆ f64       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 3539699   ┆ 62795     ┆ true  ┆ 36         ┆ … ┆ 3          ┆ 1.0        ┆ 2.0       ┆ 1.272727  │\n",
       "│ 3518450   ┆ 60537     ┆ true  ┆ 36         ┆ … ┆ 4          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ 3483976   ┆ 55341     ┆ true  ┆ 36         ┆ … ┆ 1          ┆ 1.0        ┆ 2.0       ┆ 1.6       │\n",
       "│ 653973    ┆ 42620     ┆ false ┆ 36         ┆ … ┆ 5          ┆ 1.0        ┆ 1.0       ┆ 1.0       │\n",
       "│ 1421964   ┆ 36948     ┆ true  ┆ 36         ┆ … ┆ 10         ┆ 1.0        ┆ 4.0       ┆ 1.622951  │\n",
       "└───────────┴───────────┴───────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 23)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>profileId</th><th>companyID</th><th>sex</th><th>nationality</th><th>frequentFlyer</th><th>isVip</th><th>bySelf</th><th>corporateTarrifCode</th><th>ff_normalized</th><th>total_searches</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>most_common_departure_airport</th><th>unique_departure_airports</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>min_cabin_class</th><th>max_cabin_class</th><th>avg_cabin_class</th></tr><tr><td>i64</td><td>i64</td><td>bool</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>i64</td><td>str</td><td>u32</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>3539699</td><td>62795</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>139</td><td>&quot;&quot;</td><td>11</td><td>0.0</td><td>1</td><td>5</td><td>39</td><td>32.818182</td><td>39.0</td><td>&quot;NER&quot;</td><td>1</td><td>&quot;S7&quot;</td><td>3</td><td>1.0</td><td>2.0</td><td>1.272727</td></tr><tr><td>3518450</td><td>60537</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>86</td><td>1.0</td><td>1</td><td>4</td><td>5</td><td>4.209302</td><td>4.0</td><td>&quot;PEE&quot;</td><td>1</td><td>&quot;DP&quot;</td><td>4</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3483976</td><td>55341</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>175</td><td>&quot;&quot;</td><td>20</td><td>1.0</td><td>1</td><td>3</td><td>3</td><td>3.0</td><td>3.0</td><td>&quot;SVO&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>2.0</td><td>1.6</td></tr><tr><td>653973</td><td>42620</td><td>false</td><td>36</td><td>&quot;SU&quot;</td><td>true</td><td>true</td><td>108</td><td>&quot;SU&quot;</td><td>1819</td><td>1.0</td><td>1</td><td>5</td><td>6</td><td>5.410665</td><td>5.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>5</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>1421964</td><td>36948</td><td>true</td><td>36</td><td>&quot;SU/S7&quot;</td><td>false</td><td>true</td><td>153</td><td>&quot;SU/S7&quot;</td><td>244</td><td>0.0</td><td>1</td><td>18</td><td>19</td><td>18.217213</td><td>18.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>10</td><td>1.0</td><td>4.0</td><td>1.622951</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:40:49.683515Z",
     "start_time": "2025-08-14T17:40:49.669143Z"
    }
   },
   "cell_type": "code",
   "source": "cust_data = convert_columns_to_snake_case(cust_data)",
   "id": "790cc136939b6147",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (20, 23)\n",
       "┌────────────┬────────────┬───────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ profile_id ┆ company_id ┆ sex   ┆ nationalit ┆ … ┆ unique_ca ┆ min_cabin ┆ max_cabin ┆ avg_cabin │\n",
       "│ ---        ┆ ---        ┆ ---   ┆ y          ┆   ┆ rriers_us ┆ _class    ┆ _class    ┆ _class    │\n",
       "│ i64        ┆ i64        ┆ bool  ┆ ---        ┆   ┆ ed        ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆       ┆ i64        ┆   ┆ ---       ┆ f64       ┆ f64       ┆ f64       │\n",
       "│            ┆            ┆       ┆            ┆   ┆ u32       ┆           ┆           ┆           │\n",
       "╞════════════╪════════════╪═══════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 3539699    ┆ 62795      ┆ true  ┆ 36         ┆ … ┆ 3         ┆ 1.0       ┆ 2.0       ┆ 1.272727  │\n",
       "│ 3518450    ┆ 60537      ┆ true  ┆ 36         ┆ … ┆ 4         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3483976    ┆ 55341      ┆ true  ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 2.0       ┆ 1.6       │\n",
       "│ 653973     ┆ 42620      ┆ false ┆ 36         ┆ … ┆ 5         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 1421964    ┆ 36948      ┆ true  ┆ 36         ┆ … ┆ 10        ┆ 1.0       ┆ 4.0       ┆ 1.622951  │\n",
       "│ …          ┆ …          ┆ …     ┆ …          ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 3094797    ┆ 42622      ┆ true  ┆ 36         ┆ … ┆ 7         ┆ 1.0       ┆ 2.0       ┆ 1.092827  │\n",
       "│ 1244613    ┆ 24728      ┆ true  ┆ 36         ┆ … ┆ 10        ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 1076167    ┆ 43648      ┆ false ┆ 36         ┆ … ┆ 6         ┆ 1.0       ┆ 4.0       ┆ 2.449254  │\n",
       "│ 3531306    ┆ 60628      ┆ true  ┆ 36         ┆ … ┆ 2         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3422638    ┆ 42622      ┆ true  ┆ 36         ┆ … ┆ 8         ┆ 1.0       ┆ 2.0       ┆ 1.316327  │\n",
       "└────────────┴────────────┴───────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 23)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>profile_id</th><th>company_id</th><th>sex</th><th>nationality</th><th>frequent_flyer</th><th>is_vip</th><th>by_self</th><th>corporate_tarrif_code</th><th>ff_normalized</th><th>total_searches</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>most_common_departure_airport</th><th>unique_departure_airports</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>min_cabin_class</th><th>max_cabin_class</th><th>avg_cabin_class</th></tr><tr><td>i64</td><td>i64</td><td>bool</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>i64</td><td>str</td><td>u32</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>3539699</td><td>62795</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>139</td><td>&quot;&quot;</td><td>11</td><td>0.0</td><td>1</td><td>5</td><td>39</td><td>32.818182</td><td>39.0</td><td>&quot;NER&quot;</td><td>1</td><td>&quot;S7&quot;</td><td>3</td><td>1.0</td><td>2.0</td><td>1.272727</td></tr><tr><td>3518450</td><td>60537</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>86</td><td>1.0</td><td>1</td><td>4</td><td>5</td><td>4.209302</td><td>4.0</td><td>&quot;PEE&quot;</td><td>1</td><td>&quot;DP&quot;</td><td>4</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3483976</td><td>55341</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>175</td><td>&quot;&quot;</td><td>20</td><td>1.0</td><td>1</td><td>3</td><td>3</td><td>3.0</td><td>3.0</td><td>&quot;SVO&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>2.0</td><td>1.6</td></tr><tr><td>653973</td><td>42620</td><td>false</td><td>36</td><td>&quot;SU&quot;</td><td>true</td><td>true</td><td>108</td><td>&quot;SU&quot;</td><td>1819</td><td>1.0</td><td>1</td><td>5</td><td>6</td><td>5.410665</td><td>5.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>5</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>1421964</td><td>36948</td><td>true</td><td>36</td><td>&quot;SU/S7&quot;</td><td>false</td><td>true</td><td>153</td><td>&quot;SU/S7&quot;</td><td>244</td><td>0.0</td><td>1</td><td>18</td><td>19</td><td>18.217213</td><td>18.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>10</td><td>1.0</td><td>4.0</td><td>1.622951</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>3094797</td><td>42622</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>237</td><td>0.0</td><td>2</td><td>0</td><td>2</td><td>1.274262</td><td>1.0</td><td>&quot;SVX&quot;</td><td>5</td><td>&quot;U6&quot;</td><td>7</td><td>1.0</td><td>2.0</td><td>1.092827</td></tr><tr><td>1244613</td><td>24728</td><td>true</td><td>36</td><td>&quot;SU/UT&quot;</td><td>false</td><td>true</td><td>161</td><td>&quot;SU/UT&quot;</td><td>110</td><td>0.309091</td><td>6</td><td>2</td><td>9</td><td>3.781818</td><td>3.0</td><td>&quot;VKO&quot;</td><td>5</td><td>&quot;SU&quot;</td><td>10</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>1076167</td><td>43648</td><td>false</td><td>36</td><td>&quot;SU&quot;</td><td>false</td><td>true</td><td>null</td><td>&quot;SU&quot;</td><td>1005</td><td>1.0</td><td>1</td><td>18</td><td>19</td><td>18.39005</td><td>18.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>6</td><td>1.0</td><td>4.0</td><td>2.449254</td></tr><tr><td>3531306</td><td>60628</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>12</td><td>0.0</td><td>1</td><td>6</td><td>7</td><td>6.166667</td><td>6.0</td><td>&quot;NSK&quot;</td><td>1</td><td>&quot;S7&quot;</td><td>2</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3422638</td><td>42622</td><td>true</td><td>36</td><td>null</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>98</td><td>0.0</td><td>1</td><td>2</td><td>3</td><td>2.357143</td><td>2.0</td><td>&quot;OVB&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>8</td><td>1.0</td><td>2.0</td><td>1.316327</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:43:24.110847Z",
     "start_time": "2025-08-14T17:43:24.102261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'cust_data shape {cust_data.shape}')\n",
    "cust_data.head(20)"
   ],
   "id": "d438900d1643210a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_data shape (32922, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shape: (20, 22)\n",
       "┌────────────┬────────────┬───────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ profile_id ┆ company_id ┆ sex   ┆ nationalit ┆ … ┆ unique_ca ┆ min_cabin ┆ max_cabin ┆ avg_cabin │\n",
       "│ ---        ┆ ---        ┆ ---   ┆ y          ┆   ┆ rriers_us ┆ _class    ┆ _class    ┆ _class    │\n",
       "│ i64        ┆ i64        ┆ bool  ┆ ---        ┆   ┆ ed        ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆       ┆ i64        ┆   ┆ ---       ┆ f64       ┆ f64       ┆ f64       │\n",
       "│            ┆            ┆       ┆            ┆   ┆ u32       ┆           ┆           ┆           │\n",
       "╞════════════╪════════════╪═══════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 3539699    ┆ 62795      ┆ true  ┆ 36         ┆ … ┆ 3         ┆ 1.0       ┆ 2.0       ┆ 1.272727  │\n",
       "│ 3518450    ┆ 60537      ┆ true  ┆ 36         ┆ … ┆ 4         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3483976    ┆ 55341      ┆ true  ┆ 36         ┆ … ┆ 1         ┆ 1.0       ┆ 2.0       ┆ 1.6       │\n",
       "│ 653973     ┆ 42620      ┆ false ┆ 36         ┆ … ┆ 5         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 1421964    ┆ 36948      ┆ true  ┆ 36         ┆ … ┆ 10        ┆ 1.0       ┆ 4.0       ┆ 1.622951  │\n",
       "│ …          ┆ …          ┆ …     ┆ …          ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 3094797    ┆ 42622      ┆ true  ┆ 36         ┆ … ┆ 7         ┆ 1.0       ┆ 2.0       ┆ 1.092827  │\n",
       "│ 1244613    ┆ 24728      ┆ true  ┆ 36         ┆ … ┆ 10        ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 1076167    ┆ 43648      ┆ false ┆ 36         ┆ … ┆ 6         ┆ 1.0       ┆ 4.0       ┆ 2.449254  │\n",
       "│ 3531306    ┆ 60628      ┆ true  ┆ 36         ┆ … ┆ 2         ┆ 1.0       ┆ 1.0       ┆ 1.0       │\n",
       "│ 3422638    ┆ 42622      ┆ true  ┆ 36         ┆ … ┆ 8         ┆ 1.0       ┆ 2.0       ┆ 1.316327  │\n",
       "└────────────┴────────────┴───────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 22)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>profile_id</th><th>company_id</th><th>sex</th><th>nationality</th><th>is_vip</th><th>by_self</th><th>corporate_tarrif_code</th><th>ff_normalized</th><th>total_searches</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>most_common_departure_airport</th><th>unique_departure_airports</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>min_cabin_class</th><th>max_cabin_class</th><th>avg_cabin_class</th></tr><tr><td>i64</td><td>i64</td><td>bool</td><td>i64</td><td>bool</td><td>bool</td><td>i64</td><td>str</td><td>u32</td><td>f64</td><td>u32</td><td>i32</td><td>i32</td><td>f64</td><td>f64</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>3539699</td><td>62795</td><td>true</td><td>36</td><td>false</td><td>true</td><td>139</td><td>&quot;&quot;</td><td>11</td><td>0.0</td><td>1</td><td>5</td><td>39</td><td>32.818182</td><td>39.0</td><td>&quot;NER&quot;</td><td>1</td><td>&quot;S7&quot;</td><td>3</td><td>1.0</td><td>2.0</td><td>1.272727</td></tr><tr><td>3518450</td><td>60537</td><td>true</td><td>36</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>86</td><td>1.0</td><td>1</td><td>4</td><td>5</td><td>4.209302</td><td>4.0</td><td>&quot;PEE&quot;</td><td>1</td><td>&quot;DP&quot;</td><td>4</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3483976</td><td>55341</td><td>true</td><td>36</td><td>false</td><td>true</td><td>175</td><td>&quot;&quot;</td><td>20</td><td>1.0</td><td>1</td><td>3</td><td>3</td><td>3.0</td><td>3.0</td><td>&quot;SVO&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>1</td><td>1.0</td><td>2.0</td><td>1.6</td></tr><tr><td>653973</td><td>42620</td><td>false</td><td>36</td><td>true</td><td>true</td><td>108</td><td>&quot;SU&quot;</td><td>1819</td><td>1.0</td><td>1</td><td>5</td><td>6</td><td>5.410665</td><td>5.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>5</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>1421964</td><td>36948</td><td>true</td><td>36</td><td>false</td><td>true</td><td>153</td><td>&quot;SU/S7&quot;</td><td>244</td><td>0.0</td><td>1</td><td>18</td><td>19</td><td>18.217213</td><td>18.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>10</td><td>1.0</td><td>4.0</td><td>1.622951</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>3094797</td><td>42622</td><td>true</td><td>36</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>237</td><td>0.0</td><td>2</td><td>0</td><td>2</td><td>1.274262</td><td>1.0</td><td>&quot;SVX&quot;</td><td>5</td><td>&quot;U6&quot;</td><td>7</td><td>1.0</td><td>2.0</td><td>1.092827</td></tr><tr><td>1244613</td><td>24728</td><td>true</td><td>36</td><td>false</td><td>true</td><td>161</td><td>&quot;SU/UT&quot;</td><td>110</td><td>0.309091</td><td>6</td><td>2</td><td>9</td><td>3.781818</td><td>3.0</td><td>&quot;VKO&quot;</td><td>5</td><td>&quot;SU&quot;</td><td>10</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>1076167</td><td>43648</td><td>false</td><td>36</td><td>false</td><td>true</td><td>null</td><td>&quot;SU&quot;</td><td>1005</td><td>1.0</td><td>1</td><td>18</td><td>19</td><td>18.39005</td><td>18.0</td><td>&quot;SVO&quot;</td><td>3</td><td>&quot;SU&quot;</td><td>6</td><td>1.0</td><td>4.0</td><td>2.449254</td></tr><tr><td>3531306</td><td>60628</td><td>true</td><td>36</td><td>false</td><td>true</td><td>null</td><td>&quot;&quot;</td><td>12</td><td>0.0</td><td>1</td><td>6</td><td>7</td><td>6.166667</td><td>6.0</td><td>&quot;NSK&quot;</td><td>1</td><td>&quot;S7&quot;</td><td>2</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>3422638</td><td>42622</td><td>true</td><td>36</td><td>false</td><td>true</td><td>108</td><td>&quot;&quot;</td><td>98</td><td>0.0</td><td>1</td><td>2</td><td>3</td><td>2.357143</td><td>2.0</td><td>&quot;OVB&quot;</td><td>1</td><td>&quot;SU&quot;</td><td>8</td><td>1.0</td><td>2.0</td><td>1.316327</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Another Approach",
   "id": "18557775ffac1798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:20:12.070566Z",
     "start_time": "2025-08-14T19:20:12.027587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import umap\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FlightCustomerClusteringAnalyzer:\n",
    "    def __init__(self, df, use_existing_features=True):\n",
    "        self.df = df\n",
    "        self.use_existing_features = use_existing_features\n",
    "        self.processed_df = None\n",
    "        self.scaled_features = None\n",
    "        self.cluster_labels = None\n",
    "\n",
    "    def engineer_advanced_features(self):\n",
    "        \"\"\"Create more sophisticated features for better clustering\"\"\"\n",
    "\n",
    "        if not self.use_existing_features:\n",
    "            # Original feature engineering for raw data\n",
    "            lazy_df = self.df.lazy()\n",
    "\n",
    "            # 1. BEHAVIORAL PATTERNS\n",
    "            behavioral_features = [\n",
    "                # Search intensity patterns\n",
    "                (pl.col('total_searches') / pl.col('unique_routes_searched')).alias('search_intensity_per_route'),\n",
    "\n",
    "                # Planning behavior\n",
    "                (pl.col('max_booking_lead_days') - pl.col('min_booking_lead_days')).alias('lead_time_variance'),\n",
    "                (pl.col('avg_booking_lead_days') / pl.col('median_booking_lead_days')).alias('lead_time_skew'),\n",
    "\n",
    "                # Loyalty patterns\n",
    "                (pl.col('unique_carriers_used') / pl.col('total_searches')).alias('carrier_diversity'),\n",
    "                (pl.col('unique_departure_airports') / pl.col('total_searches')).alias('airport_diversity'),\n",
    "\n",
    "                # Service preferences\n",
    "                (pl.col('max_cabin_class') - pl.col('min_cabin_class')).alias('cabin_class_range'),\n",
    "\n",
    "                # VIP and corporate indicators combined\n",
    "                (pl.col('is_vip').cast(pl.Int8) * 2 + pl.col('corporate_tariff_code').is_not_null().cast(pl.Int8)).alias('customer_tier'),\n",
    "            ]\n",
    "\n",
    "            processed_df = lazy_df.with_columns(behavioral_features).collect()\n",
    "\n",
    "        else:\n",
    "            # Work with your existing aggregated features\n",
    "            print(\"Using existing customer features - adding enhanced behavioral patterns...\")\n",
    "\n",
    "            # Check which columns exist in your dataset\n",
    "            available_cols = self.df.columns\n",
    "\n",
    "            # Create advanced features based on what's available\n",
    "            enhanced_features = []\n",
    "\n",
    "            if all(col in available_cols for col in ['total_searches', 'unique_routes_searched']):\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('total_searches') / pl.col('unique_routes_searched').clip(1)).alias('search_intensity_per_route')\n",
    "                )\n",
    "\n",
    "            if all(col in available_cols for col in ['max_booking_lead_days', 'min_booking_lead_days']):\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('max_booking_lead_days') - pl.col('min_booking_lead_days')).alias('lead_time_variance')\n",
    "                )\n",
    "\n",
    "            if all(col in available_cols for col in ['avg_booking_lead_days', 'median_booking_lead_days']):\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('avg_booking_lead_days') / pl.col('median_booking_lead_days').clip(1)).alias('lead_time_skew')\n",
    "                )\n",
    "\n",
    "            if all(col in available_cols for col in ['unique_carriers_used', 'total_searches']):\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('unique_carriers_used') / pl.col('total_searches').clip(1)).alias('carrier_diversity')\n",
    "                )\n",
    "\n",
    "            if all(col in available_cols for col in ['unique_departure_airports', 'total_searches']):\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('unique_departure_airports') / pl.col('total_searches').clip(1)).alias('airport_diversity')\n",
    "                )\n",
    "\n",
    "            if all(col in available_cols for col in ['max_cabin_class', 'min_cabin_class']):\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('max_cabin_class') - pl.col('min_cabin_class')).alias('cabin_class_range')\n",
    "                )\n",
    "\n",
    "            if 'is_vip' in available_cols and 'corporate_tariff_code' in available_cols:\n",
    "                enhanced_features.append(\n",
    "                    (pl.col('is_vip').cast(pl.Int8) * 2 + pl.col('corporate_tariff_code').is_not_null().cast(pl.Int8)).alias('customer_tier')\n",
    "                )\n",
    "\n",
    "            # Apply enhancements\n",
    "            if enhanced_features:\n",
    "                processed_df = self.df.with_columns(enhanced_features)\n",
    "            else:\n",
    "                processed_df = self.df.clone()\n",
    "                print(\"Warning: Could not create enhanced features - using original dataset\")\n",
    "\n",
    "        return processed_df\n",
    "\n",
    "    def create_interaction_features(self, df):\n",
    "        \"\"\"Create interaction features between key variables\"\"\"\n",
    "\n",
    "        interaction_features = []\n",
    "\n",
    "        # VIP interactions\n",
    "        vip_interactions = [\n",
    "            'search_intensity_per_route * is_vip',\n",
    "            'carrier_diversity * is_vip',\n",
    "            'avg_cabin_class * is_vip'\n",
    "        ]\n",
    "\n",
    "        # Corporate interactions\n",
    "        corp_interactions = [\n",
    "            'total_searches * (corporate_tariff_code.is_not_null())',\n",
    "            'roundtrip_preference * (corporate_tariff_code.is_not_null())',\n",
    "            'lead_time_variance * (corporate_tariff_code.is_not_null())'\n",
    "        ]\n",
    "\n",
    "        # Convert to pandas for easier interaction creation\n",
    "        df_pd = df.to_pandas()\n",
    "\n",
    "        # Create VIP interactions\n",
    "        df_pd['vip_search_intensity'] = df_pd['search_intensity_per_route'] * df_pd['is_vip']\n",
    "        df_pd['vip_carrier_diversity'] = df_pd['carrier_diversity'] * df_pd['is_vip']\n",
    "        df_pd['vip_cabin_preference'] = df_pd['avg_cabin_class'] * df_pd['is_vip']\n",
    "\n",
    "        # Create corporate interactions\n",
    "        has_corp = df_pd['corporate_tariff_code'].notna().astype(int)\n",
    "        df_pd['corp_search_volume'] = df_pd['total_searches'] * has_corp\n",
    "        df_pd['corp_roundtrip_pref'] = df_pd['roundtrip_preference'] * has_corp\n",
    "        df_pd['corp_planning_variance'] = df_pd['lead_time_variance'] * has_corp\n",
    "\n",
    "        return pl.from_pandas(df_pd)\n",
    "\n",
    "    def remove_outliers(self, df, method='isolation_forest', contamination=0.05):\n",
    "        \"\"\"Remove outliers using various methods\"\"\"\n",
    "\n",
    "        # Get numeric columns only\n",
    "        numeric_cols = [col for col, dtype in df.schema.items() if dtype.is_numeric()]\n",
    "        df_numeric = df.select(numeric_cols)\n",
    "\n",
    "        if method == 'isolation_forest':\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            outlier_labels = iso_forest.fit_predict(df_numeric.to_pandas())\n",
    "            mask = outlier_labels == 1\n",
    "\n",
    "        elif method == 'zscore':\n",
    "            # Remove rows where any feature has |z-score| > 3\n",
    "            df_pd = df_numeric.to_pandas()\n",
    "            z_scores = np.abs(zscore(df_pd, nan_policy='omit'))\n",
    "            mask = (z_scores < 3).all(axis=1)\n",
    "\n",
    "        elif method == 'iqr':\n",
    "            # Remove rows outside 1.5*IQR for any feature\n",
    "            df_pd = df_numeric.to_pandas()\n",
    "            Q1 = df_pd.quantile(0.25)\n",
    "            Q3 = df_pd.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            mask = ~((df_pd < (Q1 - 1.5 * IQR)) | (df_pd > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "        cleaned_df = df.filter(pl.Series('mask', mask))\n",
    "        print(f\"Removed {len(df) - len(cleaned_df):,} outliers ({(len(df) - len(cleaned_df))/len(df)*100:.1f}%)\")\n",
    "\n",
    "        return cleaned_df\n",
    "\n",
    "    def advanced_feature_encoding(self, df):\n",
    "        \"\"\"More sophisticated encoding for categorical features\"\"\"\n",
    "\n",
    "        # Get categorical columns\n",
    "        categorical_cols = [col for col, dtype in df.schema.items() if not dtype.is_numeric() and col not in ['id', 'ranker_id', 'request_date']]\n",
    "\n",
    "        encoded_df = df.clone()\n",
    "        encoders = {}\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            if col == 'ff_normalized':\n",
    "                # Special handling for frequent flyer programs (list feature)\n",
    "                # Create binary features for most common programs\n",
    "                all_programs = []\n",
    "                for programs in df[col].to_list():\n",
    "                    if isinstance(programs, list):\n",
    "                        all_programs.extend(programs)\n",
    "\n",
    "                from collections import Counter\n",
    "                top_programs = Counter(all_programs).most_common(10)\n",
    "\n",
    "                for program, _ in top_programs:\n",
    "                    encoded_df = encoded_df.with_columns([\n",
    "                        pl.col(col).map_elements(\n",
    "                            lambda x: 1 if isinstance(x, list) and program in x else 0,\n",
    "                            return_dtype=pl.Int8\n",
    "                        ).alias(f'ff_{program}')\n",
    "                    ])\n",
    "\n",
    "                # Add count of total programs\n",
    "                encoded_df = encoded_df.with_columns([\n",
    "                    pl.col(col).map_elements(\n",
    "                        lambda x: len(x) if isinstance(x, list) else 0,\n",
    "                        return_dtype=pl.Int8\n",
    "                    ).alias('ff_program_count')\n",
    "                ])\n",
    "\n",
    "            else:\n",
    "                # Target encoding for high-cardinality categoricals like airports/carriers\n",
    "                if col in ['most_common_departure_airport', 'most_common_carrier']:\n",
    "                    # Use frequency encoding\n",
    "                    value_counts = df[col].value_counts()\n",
    "                    freq_map = {row[0]: row[1] for row in value_counts.rows()}\n",
    "\n",
    "                    encoded_df = encoded_df.with_columns([\n",
    "                        pl.col(col).replace(freq_map, default=0).alias(f'{col}_frequency')\n",
    "                    ])\n",
    "                else:\n",
    "                    # Standard label encoding for low-cardinality features\n",
    "                    unique_values = df[col].fill_null('MISSING').unique().sort().to_list()\n",
    "                    encoders[col] = {val: idx for idx, val in enumerate(unique_values)}\n",
    "\n",
    "                    encoded_df = encoded_df.with_columns([\n",
    "                        pl.col(col).fill_null('MISSING').replace(encoders[col]).alias(f'{col}_encoded')\n",
    "\n",
    "                    ])\n",
    "\n",
    "        # Remove original categorical columns\n",
    "        final_df = encoded_df.select(pl.exclude(categorical_cols + ['id', 'ranker_id', 'request_date']))\n",
    "\n",
    "        return final_df.fill_null(0), encoders\n",
    "\n",
    "    def dimensionality_reduction(self, scaled_features, method='pca', n_components=50):\n",
    "        \"\"\"Apply dimensionality reduction before clustering\"\"\"\n",
    "\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "        elif method == 'truncated_svd':\n",
    "            reducer = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "\n",
    "        elif method == 'umap':\n",
    "            reducer = umap.UMAP(n_components=n_components, random_state=42, n_neighbors=15)\n",
    "\n",
    "        reduced_features = reducer.fit_transform(scaled_features)\n",
    "\n",
    "        if hasattr(reducer, 'explained_variance_ratio_'):\n",
    "            total_variance = reducer.explained_variance_ratio_.sum()\n",
    "            print(f\"{method.upper()} retained {total_variance:.3f} of total variance with {n_components} components\")\n",
    "\n",
    "        return reduced_features, reducer\n",
    "\n",
    "    def alternative_clustering_methods(self, features):\n",
    "        \"\"\"Try different clustering algorithms\"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # 1. Gaussian Mixture Models\n",
    "        print(\"Testing Gaussian Mixture Models...\")\n",
    "        best_gmm_score = -1\n",
    "        best_gmm_n = 0\n",
    "\n",
    "        for n_clusters in [15, 20, 25, 30, 35, 40]:\n",
    "            gmm = GaussianMixture(n_components=n_clusters, random_state=42, covariance_type='full')\n",
    "            labels = gmm.fit_predict(features)\n",
    "\n",
    "            if len(set(labels)) > 1:  # Ensure we have multiple clusters\n",
    "                score = silhouette_score(features, labels)\n",
    "                if score > best_gmm_score:\n",
    "                    best_gmm_score = score\n",
    "                    best_gmm_n = n_clusters\n",
    "\n",
    "        # Fit best GMM\n",
    "        best_gmm = GaussianMixture(n_components=best_gmm_n, random_state=42, covariance_type='full')\n",
    "        gmm_labels = best_gmm.fit_predict(features)\n",
    "        results['gmm'] = {\n",
    "            'labels': gmm_labels,\n",
    "            'silhouette': silhouette_score(features, gmm_labels),\n",
    "            'n_clusters': len(set(gmm_labels))\n",
    "        }\n",
    "\n",
    "        # 2. DBSCAN\n",
    "        print(\"Testing DBSCAN...\")\n",
    "        # Test different eps values\n",
    "        best_dbscan_score = -1\n",
    "        best_dbscan_eps = 0\n",
    "\n",
    "        for eps in [0.3, 0.5, 0.7, 1.0, 1.5]:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=50)\n",
    "            labels = dbscan.fit_predict(features)\n",
    "\n",
    "            if len(set(labels)) > 1 and -1 not in labels:  # Ensure valid clustering\n",
    "                score = silhouette_score(features, labels)\n",
    "                if score > best_dbscan_score:\n",
    "                    best_dbscan_score = score\n",
    "                    best_dbscan_eps = eps\n",
    "\n",
    "        if best_dbscan_eps > 0:\n",
    "            best_dbscan = DBSCAN(eps=best_dbscan_eps, min_samples=50)\n",
    "            dbscan_labels = best_dbscan.fit_predict(features)\n",
    "            results['dbscan'] = {\n",
    "                'labels': dbscan_labels,\n",
    "                'silhouette': silhouette_score(features, dbscan_labels),\n",
    "                'n_clusters': len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "            }\n",
    "\n",
    "        # 3. Agglomerative Clustering\n",
    "        print(\"Testing Agglomerative Clustering...\")\n",
    "        best_agg_score = -1\n",
    "        best_agg_n = 0\n",
    "\n",
    "        for n_clusters in [10, 15, 20, 25, 30, 35]:\n",
    "            agg = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "            labels = agg.fit_predict(features)\n",
    "\n",
    "            score = silhouette_score(features, labels)\n",
    "            if score > best_agg_score:\n",
    "                best_agg_score = score\n",
    "                best_agg_n = n_clusters\n",
    "\n",
    "        best_agg = AgglomerativeClustering(n_clusters=best_agg_n, linkage='ward')\n",
    "        agg_labels = best_agg.fit_predict(features)\n",
    "        results['agglomerative'] = {\n",
    "            'labels': agg_labels,\n",
    "            'silhouette': silhouette_score(features, agg_labels),\n",
    "            'n_clusters': len(set(agg_labels))\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def comprehensive_clustering_pipeline(self):\n",
    "        \"\"\"Complete pipeline with all improvements\"\"\"\n",
    "\n",
    "        print(\"🚀 Starting comprehensive clustering analysis...\")\n",
    "\n",
    "        # 1. Feature Engineering\n",
    "        print(\"\\n1️⃣ Advanced feature engineering...\")\n",
    "        enhanced_df = self.engineer_advanced_features()\n",
    "        enhanced_df = self.create_interaction_features(enhanced_df)\n",
    "\n",
    "        # 2. Remove outliers\n",
    "        print(\"\\n2️⃣ Removing outliers...\")\n",
    "        cleaned_df = self.remove_outliers(enhanced_df, method='isolation_forest')\n",
    "\n",
    "        # 3. Advanced encoding\n",
    "        print(\"\\n3️⃣ Advanced feature encoding...\")\n",
    "        encoded_df, encoders = self.advanced_feature_encoding(cleaned_df)\n",
    "\n",
    "        # 4. Feature scaling with robust scaler\n",
    "        print(\"\\n4️⃣ Feature scaling...\")\n",
    "        scaler = RobustScaler()  # Less sensitive to outliers than StandardScaler\n",
    "        scaled_features = scaler.fit_transform(encoded_df.to_pandas())\n",
    "\n",
    "        # 5. Dimensionality reduction\n",
    "        print(\"\\n5️⃣ Dimensionality reduction...\")\n",
    "        reduced_features, reducer = self.dimensionality_reduction(scaled_features, method='pca', n_components=30)\n",
    "\n",
    "        # 6. Try alternative clustering methods\n",
    "        print(\"\\n6️⃣ Testing clustering algorithms...\")\n",
    "        clustering_results = self.alternative_clustering_methods(reduced_features)\n",
    "\n",
    "        # 7. Compare results\n",
    "        print(\"\\n🏆 CLUSTERING RESULTS COMPARISON:\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for method, results in clustering_results.items():\n",
    "            print(f\"{method.upper():15s} | Silhouette: {results['silhouette']:.4f} | Clusters: {results['n_clusters']:2d}\")\n",
    "\n",
    "        # Select best method\n",
    "        best_method = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])\n",
    "        print(f\"\\n🥇 Best method: {best_method[0].upper()} (Silhouette: {best_method[1]['silhouette']:.4f})\")\n",
    "\n",
    "        # Store results\n",
    "        self.processed_df = encoded_df\n",
    "        self.scaled_features = reduced_features\n",
    "        self.cluster_labels = best_method[1]['labels']\n",
    "        self.encoders = encoders\n",
    "        self.scaler = scaler\n",
    "        self.reducer = reducer\n",
    "\n",
    "        return {\n",
    "            'cleaned_df': cleaned_df,\n",
    "            'processed_df': encoded_df,\n",
    "            'scaled_features': reduced_features,\n",
    "            'cluster_labels': best_method[1]['labels'],\n",
    "            'best_method': best_method[0],\n",
    "            'best_silhouette': best_method[1]['silhouette'],\n",
    "            'encoders': encoders,\n",
    "            'scaler': scaler,\n",
    "            'reducer': reducer\n",
    "        }\n",
    "\n",
    "    def analyze_clusters(self, original_df=None):\n",
    "        \"\"\"Analyze the final clusters\"\"\"\n",
    "        if self.cluster_labels is None:\n",
    "            print(\"No clustering results found. Run comprehensive_clustering_pipeline first.\")\n",
    "            return\n",
    "\n",
    "        # Add cluster labels to original dataframe for analysis\n",
    "        if original_df is not None:\n",
    "            # Check if sizes match\n",
    "            if len(self.cluster_labels) != len(original_df):\n",
    "                print(f\"Warning: Cluster labels size ({len(self.cluster_labels)}) doesn't match DataFrame size ({len(original_df)})\")\n",
    "                print(\"This likely happened because outliers were removed during clustering.\")\n",
    "\n",
    "                # Create a mask of rows that were kept after outlier removal\n",
    "                if hasattr(self, 'outlier_indices'):\n",
    "                    # If we tracked which rows were removed\n",
    "                    mask = pl.Series(pl.UInt32, range(len(original_df))).is_in(self.outlier_indices)\n",
    "                    analysis_df = original_df.filter(mask)\n",
    "                else:\n",
    "                    # If we didn't track indices, just use the first n rows\n",
    "                    # This is not ideal but will avoid the error\n",
    "                    print(\"Warning: Cannot map exact rows. Using first rows of original dataframe.\")\n",
    "                    analysis_df = original_df.head(len(self.cluster_labels))\n",
    "            else:\n",
    "                # Sizes match, proceed normally\n",
    "                analysis_df = original_df\n",
    "\n",
    "            # Now add cluster labels safely\n",
    "            analysis_df = analysis_df.with_columns(pl.Series('cluster', self.cluster_labels))\n",
    "\n",
    "            # Cluster profiling\n",
    "            print(\"\\n📊 CLUSTER PROFILES:\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "            cluster_profiles = analysis_df.group_by('cluster').agg([\n",
    "                pl.col('total_searches').mean().alias('avg_searches'),\n",
    "                pl.col('is_vip').mean().alias('vip_rate'),\n",
    "                pl.col('roundtrip_preference').mean().alias('roundtrip_rate'),\n",
    "                pl.col('avg_booking_lead_days').mean().alias('avg_lead_days'),\n",
    "                pl.col('unique_carriers_used').mean().alias('avg_carriers'),\n",
    "                pl.count().alias('size')\n",
    "            ]).sort('cluster')\n",
    "\n",
    "            print(cluster_profiles)\n",
    "\n",
    "        return self.cluster_labels\n",
    "\n",
    "\n"
   ],
   "id": "61813124116936f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:47:16.244407Z",
     "start_time": "2025-08-14T19:38:59.541331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# USAGE OPTIONS:\n",
    "\n",
    "# OPTION 1: Use your existing customer-level aggregated features (RECOMMENDED)\n",
    "\n",
    "# Initialize analyzer for existing features\n",
    "analyzer = FlightCustomerClusteringAnalyzer(cust_data.drop('profile_id'), use_existing_features=True)\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = analyzer.comprehensive_clustering_pipeline()\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_labels = analyzer.analyze_clusters(results['cleaned_df'])\n"
   ],
   "id": "efdf6c7d654579d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive clustering analysis...\n",
      "\n",
      "1️⃣ Advanced feature engineering...\n",
      "Using existing customer features - adding enhanced behavioral patterns...\n",
      "\n",
      "2️⃣ Removing outliers...\n",
      "Removed 1,647 outliers (5.0%)\n",
      "\n",
      "3️⃣ Advanced feature encoding...\n",
      "\n",
      "4️⃣ Feature scaling...\n",
      "\n",
      "5️⃣ Dimensionality reduction...\n",
      "PCA retained 1.000 of total variance with 30 components\n",
      "\n",
      "6️⃣ Testing clustering algorithms...\n",
      "Testing Gaussian Mixture Models...\n",
      "Testing DBSCAN...\n",
      "Testing Agglomerative Clustering...\n",
      "\n",
      "🏆 CLUSTERING RESULTS COMPARISON:\n",
      "============================================================\n",
      "GMM             | Silhouette: -0.1461 | Clusters: 35\n",
      "AGGLOMERATIVE   | Silhouette: 0.8143 | Clusters: 10\n",
      "\n",
      "🥇 Best method: AGGLOMERATIVE (Silhouette: 0.8143)\n",
      "\n",
      "📊 CLUSTER PROFILES:\n",
      "==================================================\n",
      "shape: (10, 7)\n",
      "┌─────────┬──────────────┬──────────┬────────────────┬───────────────┬──────────────┬───────┐\n",
      "│ cluster ┆ avg_searches ┆ vip_rate ┆ roundtrip_rate ┆ avg_lead_days ┆ avg_carriers ┆ size  │\n",
      "│ ---     ┆ ---          ┆ ---      ┆ ---            ┆ ---           ┆ ---          ┆ ---   │\n",
      "│ i64     ┆ f64          ┆ f64      ┆ f64            ┆ f64           ┆ f64          ┆ u32   │\n",
      "╞═════════╪══════════════╪══════════╪════════════════╪═══════════════╪══════════════╪═══════╡\n",
      "│ 0       ┆ 636.702703   ┆ 1.0      ┆ 0.837071       ┆ 14.648074     ┆ 6.351351     ┆ 74    │\n",
      "│ 1       ┆ 399.485862   ┆ 0.002989 ┆ 0.557217       ┆ 14.600093     ┆ 4.949004     ┆ 26100 │\n",
      "│ 2       ┆ 961.157895   ┆ 1.0      ┆ 0.994497       ┆ 12.911367     ┆ 6.052632     ┆ 19    │\n",
      "│ 3       ┆ 274.2625     ┆ 1.0      ┆ 0.720113       ┆ 11.108729     ┆ 4.85         ┆ 80    │\n",
      "│ 4       ┆ 645.406      ┆ 0.004    ┆ 0.642374       ┆ 15.116148     ┆ 6.442        ┆ 500   │\n",
      "│ 5       ┆ 601.329634   ┆ 0.00111  ┆ 0.696951       ┆ 16.357499     ┆ 6.238624     ┆ 901   │\n",
      "│ 6       ┆ 1917.0       ┆ 1.0      ┆ 1.0            ┆ 8.234742      ┆ 9.0          ┆ 1     │\n",
      "│ 7       ┆ 736.021525   ┆ 0.006458 ┆ 0.674568       ┆ 15.415833     ┆ 6.450185     ┆ 3252  │\n",
      "│ 8       ┆ 402.269531   ┆ 0.011719 ┆ 0.53194        ┆ 16.920806     ┆ 5.8515625    ┆ 256   │\n",
      "│ 9       ┆ 452.206522   ┆ 0.0      ┆ 0.432803       ┆ 11.26992      ┆ 6.608696     ┆ 92    │\n",
      "└─────────┴──────────────┴──────────┴────────────────┴───────────────┴──────────────┴───────┘\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:01:29.739096Z",
     "start_time": "2025-08-14T19:01:29.729246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OPTION 2: Create fresh customer aggregation from raw flight data\n",
    "\n",
    "# Create customer aggregation with richer behavioral features\n",
    "def create_enhanced_customer_features(df_raw):\n",
    "    # Advanced customer-level aggregation from raw search data\n",
    "    customer_features = df_raw.group_by('profileId').agg([\n",
    "        # Basic stats\n",
    "        pl.count().alias('total_searches'),\n",
    "        pl.col('ranker_id').n_unique().alias('unique_sessions'),\n",
    "\n",
    "        # Route behavior\n",
    "        pl.col('searchRoute').n_unique().alias('unique_routes'),\n",
    "        pl.col('searchRoute').str.contains('/').mean().alias('roundtrip_preference'),\n",
    "        pl.col('searchRoute').drop_nulls().n_unique().alias('unique_routes_searched'),\n",
    "\n",
    "        # Pricing behavior\n",
    "        pl.col('totalPrice').mean().alias('avg_total_price'),\n",
    "        pl.col('totalPrice').median().alias('median_total_price'),\n",
    "        pl.col('totalPrice').std().alias('price_std'),\n",
    "\n",
    "        # Timing patterns\n",
    "        pl.col('legs0_departureAt').str.to_datetime().dt.hour().mode().first().alias('preferred_departure_hour'),\n",
    "\n",
    "        # Booking lead time (days between request and departure)\n",
    "        ((pl.col('legs0_departureAt').str.to_datetime() - pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1))\n",
    "        .cast(pl.Int32).mean().alias('avg_booking_lead_days'),\n",
    "        ((pl.col('legs0_departureAt').str.to_datetime() - pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1))\n",
    "        .cast(pl.Int32).median().alias('median_booking_lead_days'),\n",
    "        ((pl.col('legs0_departureAt').str.to_datetime() - pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1))\n",
    "        .cast(pl.Int32).min().alias('min_booking_lead_days'),\n",
    "        ((pl.col('legs0_departureAt').str.to_datetime() - pl.col('requestDate').cast(pl.Datetime)) / pl.duration(days=1))\n",
    "        .cast(pl.Int32).max().alias('max_booking_lead_days'),\n",
    "\n",
    "        # Airline preferences\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').mode().first().alias('most_common_carrier'),\n",
    "        pl.col('legs0_segments0_marketingCarrier_code').n_unique().alias('unique_carriers_used'),\n",
    "\n",
    "        # Airport preferences\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').mode().first().alias('most_common_departure'),\n",
    "        pl.col('legs0_segments0_departureFrom_airport_iata').n_unique().alias('unique_departure_airports'),\n",
    "\n",
    "        # Service class preferences\n",
    "        pl.col('legs0_segments0_cabinClass').mean().alias('avg_cabin_class'),\n",
    "        pl.col('legs0_segments0_cabinClass').min().alias('min_cabin_class'),\n",
    "        pl.col('legs0_segments0_cabinClass').max().alias('max_cabin_class'),\n",
    "\n",
    "        # Selection behavior (from training data)\n",
    "        pl.col('selected').sum().alias('total_bookings'),\n",
    "        pl.col('selected').mean().alias('booking_rate'),\n",
    "\n",
    "        # User attributes (should be constant per user)\n",
    "        pl.col('sex').first(),\n",
    "        pl.col('nationality').first(),\n",
    "        pl.col('isVip').first().alias('is_vip'),\n",
    "        pl.col('bySelf').first().alias('self_type'),\n",
    "        pl.col('frequentFlyer').drop_nulls().first().str.replace('- ЮТэйр ЗАО', 'UT').fill_null('').alias('ff_normalized'),\n",
    "        pl.col('corporateTariffCode').first().alias('corporate_tariff_code'),\n",
    "        pl.col('companyID').first().alias('company_id'),\n",
    "    ])\n",
    "\n",
    "    return customer_features.drop('profileId')\n"
   ],
   "id": "f134334865771d00",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/train.parquet')",
   "id": "57642e062eec60cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:02:07.003492Z",
     "start_time": "2025-08-14T19:01:33.144416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Customer Features\n",
    "cust_features = create_enhanced_customer_features(data)"
   ],
   "id": "b8ad1d29fab9e5da",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:03:44.215166Z",
     "start_time": "2025-08-14T19:03:44.193139Z"
    }
   },
   "cell_type": "code",
   "source": "cust_features.head(20)",
   "id": "aad3d4c36e29e08c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (20, 29)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ total_sea ┆ unique_se ┆ unique_ro ┆ roundtrip ┆ … ┆ self_type ┆ ff_normal ┆ corporate ┆ company_ │\n",
       "│ rches     ┆ ssions    ┆ utes      ┆ _preferen ┆   ┆ ---       ┆ ized      ┆ _tariff_c ┆ id       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ce        ┆   ┆ bool      ┆ ---       ┆ ode       ┆ ---      │\n",
       "│ u32       ┆ u32       ┆ u32       ┆ ---       ┆   ┆           ┆ str       ┆ ---       ┆ i64      │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆           ┆           ┆ i64       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 76        ┆ 1         ┆ 1         ┆ 0.0       ┆ … ┆ true      ┆           ┆ null      ┆ 42622    │\n",
       "│ 1740      ┆ 2         ┆ 2         ┆ 1.0       ┆ … ┆ true      ┆ S7        ┆ null      ┆ 54218    │\n",
       "│ 316       ┆ 1         ┆ 1         ┆ 1.0       ┆ … ┆ true      ┆ SU/S7     ┆ null      ┆ 42620    │\n",
       "│ 6079      ┆ 4         ┆ 4         ┆ 0.971377  ┆ … ┆ true      ┆ SU/S7     ┆ null      ┆ 43648    │\n",
       "│ 69        ┆ 1         ┆ 1         ┆ 1.0       ┆ … ┆ true      ┆           ┆ null      ┆ 40253    │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 186       ┆ 2         ┆ 2         ┆ 0.0       ┆ … ┆ true      ┆           ┆ null      ┆ 54218    │\n",
       "│ 4102      ┆ 19        ┆ 11        ┆ 0.004388  ┆ … ┆ true      ┆           ┆ null      ┆ 63418    │\n",
       "│ 447       ┆ 2         ┆ 2         ┆ 1.0       ┆ … ┆ true      ┆           ┆ null      ┆ 59766    │\n",
       "│ 19        ┆ 1         ┆ 1         ┆ 1.0       ┆ … ┆ true      ┆ UT/S7     ┆ null      ┆ 61061    │\n",
       "│ 135       ┆ 8         ┆ 2         ┆ 0.0       ┆ … ┆ true      ┆ S7        ┆ null      ┆ 60734    │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 29)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>total_searches</th><th>unique_sessions</th><th>unique_routes</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>avg_total_price</th><th>median_total_price</th><th>price_std</th><th>preferred_departure_hour</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>most_common_departure</th><th>unique_departure_airports</th><th>avg_cabin_class</th><th>min_cabin_class</th><th>max_cabin_class</th><th>total_bookings</th><th>booking_rate</th><th>sex</th><th>nationality</th><th>is_vip</th><th>self_type</th><th>ff_normalized</th><th>corporate_tariff_code</th><th>company_id</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>f64</td><td>f64</td><td>i32</td><td>i32</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>bool</td><td>i64</td><td>bool</td><td>bool</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>76</td><td>1</td><td>1</td><td>0.0</td><td>1</td><td>28523.697368</td><td>19168.0</td><td>28463.330899</td><td>23</td><td>2.565789</td><td>3.0</td><td>2</td><td>3</td><td>&quot;SU&quot;</td><td>6</td><td>&quot;KUF&quot;</td><td>1</td><td>1.328947</td><td>1.0</td><td>2.0</td><td>1</td><td>0.013158</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>42622</td></tr><tr><td>1740</td><td>2</td><td>2</td><td>1.0</td><td>2</td><td>46557.237356</td><td>28648.0</td><td>35101.445349</td><td>8</td><td>6.274138</td><td>7.0</td><td>4</td><td>8</td><td>&quot;SU&quot;</td><td>7</td><td>&quot;SVO&quot;</td><td>4</td><td>1.337931</td><td>1.0</td><td>2.0</td><td>2</td><td>0.001149</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;S7&quot;</td><td>null</td><td>54218</td></tr><tr><td>316</td><td>1</td><td>1</td><td>1.0</td><td>1</td><td>17435.933544</td><td>15323.0</td><td>5943.099361</td><td>18</td><td>11.297468</td><td>11.0</td><td>11</td><td>12</td><td>&quot;SU&quot;</td><td>4</td><td>&quot;LED&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1</td><td>0.003165</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;SU/S7&quot;</td><td>null</td><td>42620</td></tr><tr><td>6079</td><td>4</td><td>4</td><td>0.971377</td><td>4</td><td>37307.698306</td><td>23034.0</td><td>30056.837301</td><td>8</td><td>11.717059</td><td>11.0</td><td>11</td><td>27</td><td>&quot;SU&quot;</td><td>11</td><td>&quot;SVO&quot;</td><td>4</td><td>1.405494</td><td>1.0</td><td>2.0</td><td>4</td><td>0.000658</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;SU/S7&quot;</td><td>null</td><td>43648</td></tr><tr><td>69</td><td>1</td><td>1</td><td>1.0</td><td>1</td><td>22435.869565</td><td>20148.0</td><td>6695.33547</td><td>5</td><td>13.782609</td><td>14.0</td><td>13</td><td>14</td><td>&quot;SU&quot;</td><td>2</td><td>&quot;VOG&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1</td><td>0.014493</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>40253</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>186</td><td>2</td><td>2</td><td>0.0</td><td>2</td><td>24369.075269</td><td>14801.0</td><td>17799.56644</td><td>10</td><td>21.860215</td><td>23.0</td><td>18</td><td>24</td><td>&quot;U6&quot;</td><td>13</td><td>&quot;AER&quot;</td><td>5</td><td>1.021505</td><td>1.0</td><td>2.0</td><td>2</td><td>0.010753</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>54218</td></tr><tr><td>4102</td><td>19</td><td>11</td><td>0.004388</td><td>11</td><td>27593.840322</td><td>18662.0</td><td>24349.139436</td><td>20</td><td>10.326914</td><td>10.0</td><td>2</td><td>32</td><td>&quot;SU&quot;</td><td>14</td><td>&quot;LED&quot;</td><td>9</td><td>1.39883</td><td>1.0</td><td>4.0</td><td>19</td><td>0.004632</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>63418</td></tr><tr><td>447</td><td>2</td><td>2</td><td>1.0</td><td>2</td><td>106717.610738</td><td>81378.0</td><td>63709.57609</td><td>20</td><td>25.836689</td><td>31.0</td><td>15</td><td>31</td><td>&quot;HU&quot;</td><td>4</td><td>&quot;SVO&quot;</td><td>1</td><td>1.131991</td><td>1.0</td><td>4.0</td><td>2</td><td>0.004474</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>59766</td></tr><tr><td>19</td><td>1</td><td>1</td><td>1.0</td><td>1</td><td>14530.947368</td><td>12595.0</td><td>4469.00146</td><td>17</td><td>9.631579</td><td>10.0</td><td>9</td><td>10</td><td>&quot;SU&quot;</td><td>2</td><td>&quot;NBC&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1</td><td>0.052632</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;UT/S7&quot;</td><td>null</td><td>61061</td></tr><tr><td>135</td><td>8</td><td>2</td><td>0.0</td><td>2</td><td>47595.37037</td><td>37695.0</td><td>28414.224536</td><td>10</td><td>20.444444</td><td>11.0</td><td>11</td><td>37</td><td>&quot;SU&quot;</td><td>3</td><td>&quot;GDX&quot;</td><td>2</td><td>1.111111</td><td>1.0</td><td>2.0</td><td>8</td><td>0.059259</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;S7&quot;</td><td>null</td><td>60734</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:29:26.036462Z",
     "start_time": "2025-08-14T19:20:26.068749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run clustering analysis\n",
    "analyzer = FlightCustomerClusteringAnalyzer(cust_features, use_existing_features=True)\n",
    "results = analyzer.comprehensive_clustering_pipeline()\n"
   ],
   "id": "ab83f603af031700",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive clustering analysis...\n",
      "\n",
      "1️⃣ Advanced feature engineering...\n",
      "Using existing customer features - adding enhanced behavioral patterns...\n",
      "\n",
      "2️⃣ Removing outliers...\n",
      "Removed 1,647 outliers (5.0%)\n",
      "\n",
      "3️⃣ Advanced feature encoding...\n",
      "\n",
      "4️⃣ Feature scaling...\n",
      "\n",
      "5️⃣ Dimensionality reduction...\n",
      "PCA retained 1.000 of total variance with 30 components\n",
      "\n",
      "6️⃣ Testing clustering algorithms...\n",
      "Testing Gaussian Mixture Models...\n",
      "Testing DBSCAN...\n",
      "Testing Agglomerative Clustering...\n",
      "\n",
      "🏆 CLUSTERING RESULTS COMPARISON:\n",
      "============================================================\n",
      "GMM             | Silhouette: -0.0259 | Clusters: 20\n",
      "AGGLOMERATIVE   | Silhouette: 0.5304 | Clusters: 10\n",
      "\n",
      "🥇 Best method: AGGLOMERATIVE (Silhouette: 0.5304)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:30:12.670072Z",
     "start_time": "2025-08-14T19:30:12.660207Z"
    }
   },
   "cell_type": "code",
   "source": "results['cleaned_df'].head(20)",
   "id": "26e90eba7764b120",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (20, 42)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ total_sea ┆ unique_se ┆ unique_ro ┆ roundtrip ┆ … ┆ vip_cabin ┆ corp_sear ┆ corp_roun ┆ corp_pla │\n",
       "│ rches     ┆ ssions    ┆ utes      ┆ _preferen ┆   ┆ _preferen ┆ ch_volume ┆ dtrip_pre ┆ nning_va │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ce        ┆   ┆ ce        ┆ ---       ┆ f         ┆ riance   │\n",
       "│ u32       ┆ u32       ┆ u32       ┆ ---       ┆   ┆ ---       ┆ i64       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆ f64       ┆           ┆ f64       ┆ i64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 76        ┆ 1         ┆ 1         ┆ 0.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 1740      ┆ 2         ┆ 2         ┆ 1.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 316       ┆ 1         ┆ 1         ┆ 1.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 6079      ┆ 4         ┆ 4         ┆ 0.971377  ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 69        ┆ 1         ┆ 1         ┆ 1.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 186       ┆ 2         ┆ 2         ┆ 0.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 447       ┆ 2         ┆ 2         ┆ 1.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 19        ┆ 1         ┆ 1         ┆ 1.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 135       ┆ 8         ┆ 2         ┆ 0.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0.0       ┆ 0        │\n",
       "│ 143       ┆ 2         ┆ 1         ┆ 1.0       ┆ … ┆ 0.0       ┆ 143       ┆ 1.0       ┆ 5        │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 42)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>total_searches</th><th>unique_sessions</th><th>unique_routes</th><th>roundtrip_preference</th><th>unique_routes_searched</th><th>avg_total_price</th><th>median_total_price</th><th>price_std</th><th>preferred_departure_hour</th><th>avg_booking_lead_days</th><th>median_booking_lead_days</th><th>min_booking_lead_days</th><th>max_booking_lead_days</th><th>most_common_carrier</th><th>unique_carriers_used</th><th>most_common_departure</th><th>unique_departure_airports</th><th>avg_cabin_class</th><th>min_cabin_class</th><th>max_cabin_class</th><th>total_bookings</th><th>booking_rate</th><th>sex</th><th>nationality</th><th>is_vip</th><th>self_type</th><th>ff_normalized</th><th>corporate_tariff_code</th><th>company_id</th><th>search_intensity_per_route</th><th>lead_time_variance</th><th>lead_time_skew</th><th>carrier_diversity</th><th>airport_diversity</th><th>cabin_class_range</th><th>customer_tier</th><th>vip_search_intensity</th><th>vip_carrier_diversity</th><th>vip_cabin_preference</th><th>corp_search_volume</th><th>corp_roundtrip_pref</th><th>corp_planning_variance</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>f64</td><td>f64</td><td>i32</td><td>i32</td><td>str</td><td>u32</td><td>str</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>bool</td><td>i64</td><td>bool</td><td>bool</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>76</td><td>1</td><td>1</td><td>0.0</td><td>1</td><td>28523.697368</td><td>19168.0</td><td>28463.330899</td><td>23</td><td>2.565789</td><td>3.0</td><td>2</td><td>3</td><td>&quot;SU&quot;</td><td>6</td><td>&quot;KUF&quot;</td><td>1</td><td>1.328947</td><td>1.0</td><td>2.0</td><td>1</td><td>0.013158</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>42622</td><td>76.0</td><td>1</td><td>0.855263</td><td>0.078947</td><td>0.013158</td><td>1.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>1740</td><td>2</td><td>2</td><td>1.0</td><td>2</td><td>46557.237356</td><td>28648.0</td><td>35101.445349</td><td>8</td><td>6.274138</td><td>7.0</td><td>4</td><td>8</td><td>&quot;SU&quot;</td><td>7</td><td>&quot;SVO&quot;</td><td>4</td><td>1.337931</td><td>1.0</td><td>2.0</td><td>2</td><td>0.001149</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;S7&quot;</td><td>null</td><td>54218</td><td>870.0</td><td>4</td><td>0.896305</td><td>0.004023</td><td>0.002299</td><td>1.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>316</td><td>1</td><td>1</td><td>1.0</td><td>1</td><td>17435.933544</td><td>15323.0</td><td>5943.099361</td><td>18</td><td>11.297468</td><td>11.0</td><td>11</td><td>12</td><td>&quot;SU&quot;</td><td>4</td><td>&quot;LED&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1</td><td>0.003165</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;SU/S7&quot;</td><td>null</td><td>42620</td><td>316.0</td><td>1</td><td>1.027043</td><td>0.012658</td><td>0.003165</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>6079</td><td>4</td><td>4</td><td>0.971377</td><td>4</td><td>37307.698306</td><td>23034.0</td><td>30056.837301</td><td>8</td><td>11.717059</td><td>11.0</td><td>11</td><td>27</td><td>&quot;SU&quot;</td><td>11</td><td>&quot;SVO&quot;</td><td>4</td><td>1.405494</td><td>1.0</td><td>2.0</td><td>4</td><td>0.000658</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;SU/S7&quot;</td><td>null</td><td>43648</td><td>1519.75</td><td>16</td><td>1.065187</td><td>0.00181</td><td>0.000658</td><td>1.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>69</td><td>1</td><td>1</td><td>1.0</td><td>1</td><td>22435.869565</td><td>20148.0</td><td>6695.33547</td><td>5</td><td>13.782609</td><td>14.0</td><td>13</td><td>14</td><td>&quot;SU&quot;</td><td>2</td><td>&quot;VOG&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1</td><td>0.014493</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>40253</td><td>69.0</td><td>1</td><td>0.984472</td><td>0.028986</td><td>0.014493</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>186</td><td>2</td><td>2</td><td>0.0</td><td>2</td><td>24369.075269</td><td>14801.0</td><td>17799.56644</td><td>10</td><td>21.860215</td><td>23.0</td><td>18</td><td>24</td><td>&quot;U6&quot;</td><td>13</td><td>&quot;AER&quot;</td><td>5</td><td>1.021505</td><td>1.0</td><td>2.0</td><td>2</td><td>0.010753</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>54218</td><td>93.0</td><td>6</td><td>0.950444</td><td>0.069892</td><td>0.026882</td><td>1.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>447</td><td>2</td><td>2</td><td>1.0</td><td>2</td><td>106717.610738</td><td>81378.0</td><td>63709.57609</td><td>20</td><td>25.836689</td><td>31.0</td><td>15</td><td>31</td><td>&quot;HU&quot;</td><td>4</td><td>&quot;SVO&quot;</td><td>1</td><td>1.131991</td><td>1.0</td><td>4.0</td><td>2</td><td>0.004474</td><td>false</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>null</td><td>59766</td><td>223.5</td><td>16</td><td>0.833442</td><td>0.008949</td><td>0.002237</td><td>3.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>19</td><td>1</td><td>1</td><td>1.0</td><td>1</td><td>14530.947368</td><td>12595.0</td><td>4469.00146</td><td>17</td><td>9.631579</td><td>10.0</td><td>9</td><td>10</td><td>&quot;SU&quot;</td><td>2</td><td>&quot;NBC&quot;</td><td>1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1</td><td>0.052632</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;UT/S7&quot;</td><td>null</td><td>61061</td><td>19.0</td><td>1</td><td>0.963158</td><td>0.105263</td><td>0.052632</td><td>0.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>135</td><td>8</td><td>2</td><td>0.0</td><td>2</td><td>47595.37037</td><td>37695.0</td><td>28414.224536</td><td>10</td><td>20.444444</td><td>11.0</td><td>11</td><td>37</td><td>&quot;SU&quot;</td><td>3</td><td>&quot;GDX&quot;</td><td>2</td><td>1.111111</td><td>1.0</td><td>2.0</td><td>8</td><td>0.059259</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;S7&quot;</td><td>null</td><td>60734</td><td>67.5</td><td>26</td><td>1.858586</td><td>0.022222</td><td>0.014815</td><td>1.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.0</td><td>0</td></tr><tr><td>143</td><td>2</td><td>1</td><td>1.0</td><td>1</td><td>71020.496503</td><td>58954.0</td><td>35094.537371</td><td>21</td><td>36.937063</td><td>39.0</td><td>34</td><td>39</td><td>&quot;S7&quot;</td><td>3</td><td>&quot;DME&quot;</td><td>2</td><td>1.167832</td><td>1.0</td><td>2.0</td><td>2</td><td>0.013986</td><td>true</td><td>36</td><td>false</td><td>true</td><td>&quot;&quot;</td><td>139.0</td><td>62795</td><td>143.0</td><td>5</td><td>0.947104</td><td>0.020979</td><td>0.013986</td><td>1.0</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>143</td><td>1.0</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:30:31.243610Z",
     "start_time": "2025-08-14T19:30:31.232156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze clusters\n",
    "cluster_labels = analyzer.analyze_clusters(results['cleaned_df'])"
   ],
   "id": "1424aaaf26142899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CLUSTER PROFILES:\n",
      "==================================================\n",
      "shape: (10, 7)\n",
      "┌─────────┬──────────────┬──────────┬────────────────┬───────────────┬──────────────┬───────┐\n",
      "│ cluster ┆ avg_searches ┆ vip_rate ┆ roundtrip_rate ┆ avg_lead_days ┆ avg_carriers ┆ size  │\n",
      "│ ---     ┆ ---          ┆ ---      ┆ ---            ┆ ---           ┆ ---          ┆ ---   │\n",
      "│ i64     ┆ f64          ┆ f64      ┆ f64            ┆ f64           ┆ f64          ┆ u32   │\n",
      "╞═════════╪══════════════╪══════════╪════════════════╪═══════════════╪══════════════╪═══════╡\n",
      "│ 0       ┆ 442.77542    ┆ 0.002867 ┆ 0.571826       ┆ 14.718218     ┆ 5.135479     ┆ 27901 │\n",
      "│ 1       ┆ 1435.933333  ┆ 1.0      ┆ 0.964778       ┆ 21.626827     ┆ 8.533333     ┆ 15    │\n",
      "│ 2       ┆ 1373.199488  ┆ 0.0      ┆ 0.850782       ┆ 13.295817     ┆ 7.219096     ┆ 1173  │\n",
      "│ 3       ┆ 205.142484   ┆ 0.0      ┆ 0.471143       ┆ 21.47051      ┆ 4.988518     ┆ 1916  │\n",
      "│ 4       ┆ 1938.75      ┆ 1.0      ┆ 1.0            ┆ 9.135501      ┆ 6.375        ┆ 8     │\n",
      "│ 5       ┆ 5332.0       ┆ 1.0      ┆ 1.0            ┆ 8.864216      ┆ 6.0          ┆ 1     │\n",
      "│ 6       ┆ 1122.52381   ┆ 1.0      ┆ 0.897767       ┆ 16.234406     ┆ 9.52381      ┆ 21    │\n",
      "│ 7       ┆ 256.833333   ┆ 1.0      ┆ 0.694172       ┆ 12.828201     ┆ 5.333333     ┆ 90    │\n",
      "│ 8       ┆ 671.708333   ┆ 1.0      ┆ 0.791794       ┆ 14.807058     ┆ 6.847222     ┆ 72    │\n",
      "│ 9       ┆ 4780.666667  ┆ 0.0      ┆ 0.969285       ┆ 9.532937      ┆ 8.153846     ┆ 78    │\n",
      "└─────────┴──────────────┴──────────┴────────────────┴───────────────┴──────────────┴───────┘\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:31:44.268887Z",
     "start_time": "2025-08-14T19:31:44.260634Z"
    }
   },
   "cell_type": "code",
   "source": "results['cleaned_df'].columns",
   "id": "d0150e3898002850",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_searches',\n",
       " 'unique_sessions',\n",
       " 'unique_routes',\n",
       " 'roundtrip_preference',\n",
       " 'unique_routes_searched',\n",
       " 'avg_total_price',\n",
       " 'median_total_price',\n",
       " 'price_std',\n",
       " 'preferred_departure_hour',\n",
       " 'avg_booking_lead_days',\n",
       " 'median_booking_lead_days',\n",
       " 'min_booking_lead_days',\n",
       " 'max_booking_lead_days',\n",
       " 'most_common_carrier',\n",
       " 'unique_carriers_used',\n",
       " 'most_common_departure',\n",
       " 'unique_departure_airports',\n",
       " 'avg_cabin_class',\n",
       " 'min_cabin_class',\n",
       " 'max_cabin_class',\n",
       " 'total_bookings',\n",
       " 'booking_rate',\n",
       " 'sex',\n",
       " 'nationality',\n",
       " 'is_vip',\n",
       " 'self_type',\n",
       " 'ff_normalized',\n",
       " 'corporate_tariff_code',\n",
       " 'company_id',\n",
       " 'search_intensity_per_route',\n",
       " 'lead_time_variance',\n",
       " 'lead_time_skew',\n",
       " 'carrier_diversity',\n",
       " 'airport_diversity',\n",
       " 'cabin_class_range',\n",
       " 'customer_tier',\n",
       " 'vip_search_intensity',\n",
       " 'vip_carrier_diversity',\n",
       " 'vip_cabin_preference',\n",
       " 'corp_search_volume',\n",
       " 'corp_roundtrip_pref',\n",
       " 'corp_planning_variance']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8bfa0a785be9eaef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
